{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import wave\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from glob import glob\n",
    "import random\n",
    "\n",
    "import struct\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "\n",
    "DATA_DIR = './speech_commands_v0.01'\n",
    "\n",
    "RUN = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEWCAYAAABBvWFzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl4VdW5x/HvLwlhnsHIJJM4D6hxqlOo1Fmxg622tWC1\n1La2dryl11712uFiB1vb23stVqu2Vhx6VawIDhgcUUABQUTCoBCReQpzkvf+sXfCSchwkuxzds7J\n+3me82QPa+/zrnMgb9Zae+8lM8M555xrqZy4A3DOOZcdPKE455yLhCcU55xzkfCE4pxzLhKeUJxz\nzkXCE4pzzrlIeEJxsZD0JUnPRnSuYknXRXGuhHPeJ+nnUZ4zCpJWShoddxzJkHSrpL/HHYdLH08o\nLmUknSnpNUlbJW2S9KqkkwHM7EEzOy/uGF3r0FoTuGuavLgDcNlJUjfgX8A3gEeAfOAsYE+ccTnn\nUsdbKC5VDgMws4fMrMLMdpnZs2a2AEDSOEmvVBWWZJKul7RU0hZJf5KkcF+upN9K2iBphaQbwvJ1\n/kEk6auSFkvaLGm6pMH1BZnQitoiaZWkcQm7e0p6WtJ2SW9IGp5w3J1h+W2S5ko6K2HfrZIekfRA\neOwiSYUJ+1dK+qGkBWHr7WFJHRL2XyJpXhjTa5KOa+zDljQ0LJ8Trt8taV3C/r9J+m64fE34+WyX\ntFzS1xPKLZZ0ScJ6nqT1kk4M109L+LzmSyqqFcPM8LzPAX1qxfiopI/DOr8k6ehw+3jgS8C/SSqT\n9FS4fYKkZeH53pX06cY+BxczM/OXvyJ/Ad2AjcD9wIVAz1r7xwGvJKwbQYumB3AIsB64INx3PfAu\nMBDoCTwfls8L9xcD14XLY4AS4EiCFvhPgdfqiXEwsB24CmgH9AZGhvvuC+M/JTzPg8DkhGO/HJbP\nA34AfAx0CPfdCuwGLgJygf8CZiUcuxJ4E+gP9AIWA9eH+04A1gGnhseODcu3Tzh2dD31+RA4KVxe\nAiwHjkzYd0K4fDEwHBBwDrATODHcdzPwYMI5LwYWh8sDws/kIoI/Rj8VrvcN978O3AG0B84OP9u/\nJ5zrq0DXcP/vgXkJ++4Dfl6rPleEn1EO8AVgB9Av7n/b/qr/5S0UlxJmtg04k+AX/93AeklTJBU0\ncNhEM9tiZh8CLwIjw+2fB+40s9VmthmY2MA5rgf+y8wWm1k58EtgZD2tlC8Cz1vQitpnZhvNbF7C\n/sfN7M3wPA8mxIOZ/T0sX25mvyX4JXl4wrGvmNlUM6sA/gYcX+u9/2BmH5nZJuCphHOPB/5sZm9Y\n0LK7n6Cb8LQG6lxlJnCOpIPD9cfC9aEECX5+GPvTZrbMAjOBZwm6IwH+AVwmqVPCZ/RQuPxlYGpY\nr0ozew6YA1wk6RDgZOA/zGyPmb0U1quamd1rZtvNbA9B0j1eUvf6KmNmj4afUaWZPQwsJUjwrpXy\nhOJSJvylPs7MBgLHEPy1+fsGDvk4YXkn0CVc7g+sStiXuFzbYODOsEtmC7CJ4C/xAXWUHQQsa0Y8\nhF1Wi8Pumy1Ad2p28dQ+tkOtLrr6zj0Y+EFV/OG5BxF8Bo2ZCRQRtA5eImi5nRO+XjazyjD2CyXN\nCi+U2ELQ4ugDYGYlBC2mS8OkchlBkqmK7YpasZ0J9Avj22xmOxLi+SDh88qVNDHswtpG0NKCWt1i\niSR9JaHrbwvBv6F6y7v4+aC8Swsze0/SfcDXGytbhzUE3V1VBjVQdhXwCzN7MInzrqIZf/GG4yX/\nBpwLLDKzSkmbCRJXS1XF/4tmHDsT+DWwOlx+BbiLoPttZhh7e+CfwFeAJ81sn6QnasX+EEE3YA7w\nbphkqmL7m5l9rfYbhy3AnpI6JySVQwhaqBC0dMYAowmSSXcg8TOr8djz8Hx3E3zGr5tZhaR5RPMZ\nuxTxFopLCUlHSPqBpIHh+iCCX1KzmnG6R4AbJQ2Q1AP4cQNl7wJ+kjDg213SFfWUfRAYLenz4eBz\nb0kj6ymbqCtQTjDOkyfpZoIupSjcDVwv6VQFOku6WFLXxg40s6XALoKuqZlht+Na4LOECYXgarv2\nYezlki4Eal++PTnc9g32t04A/k7Qcjk/bHF0kFQkaaCZfUDQ/fWfkvIlnQlcmnBsV4Kuu41AJ4Ku\nyERrgWEJ650Jksx6CC4kIGihuFbME4pLle0EA8tvSNpBkEgWEgxgN9XdBP38C4C3gakEv9Arahc0\ns8eB24HJYdfKQoKLAg4QjtVcFMa0CZjHgWMddZkOTAPeJ+jW2U3D3XBJM7M5wNeA/yb4C76E4AKG\nZM0ENprZqoR1AW+F598OfIcgSW8maDlMqRXDGoIB9k8ADydsX0XQyvh3gl/0q4Afsf/3yBcJvvNN\nwC3AAwmnfYDgsyoluMCi9h8W9wBHhd1bT5jZu8BvwzjWAscCrzbhc3AxkJlPsOUyS/hX9V1mVu/l\nwM659PMWimv1JHWUdFHYLTWA4K/fx+OOyzlXk7dQXKsXXm00EziCYIzgaeDGcIzAOddKeEJxzjkX\nCe/ycs45F4k2dR9Knz59bMiQIc06dseOHXTu3DnagFo5r3Pb4HXOfi2t79y5czeYWd/GyrWphDJk\nyBDmzJnTrGOLi4spKiqKNqBWzuvcNnids19L6yvpg8ZLeZeXc865iHhCcc45FwlPKM455yLhCcU5\n51wkPKE455yLRKwJRdK9ktZJWljPfkn6g6QSBdOlnpiwb6yC6WKXShqbvqidc87VJe4Wyn3ABQ3s\nvxAYEb7GA/8LIKkXwfOcTiWYz+IWST1TGqlzzrkGxZpQwmlCNzVQZAzwQDhV6Sygh6R+wPnAc2a2\nKZwS9jkaTkzOpYWZ8cicVewtr4w7FOfSrrXf2DiAmvNMrA631bf9AJLGE7RuKCgooLi4uFmBlJWV\nNfvYTOV1brpZa8q5a/4eXn17MZ8ekR9dYCnk33P2S1d9W3tCaTEzmwRMAigsLLTm3i3a1u6sBa9z\nc6ya9QHMX0iXPv0oKjo2usBSyL/n7Jeu+sY9htKYUmrOHz4w3FbfduecczFp7QllCvCV8Gqv04Ct\n4fSk04HzJPUMB+PPC7c555yLSaxdXpIeAoqAPpJWE1y51Q7AzO4imDv8IoJ5tXcC14T7Nkn6GTA7\nPNVtZtbQ4L5zzrkUizWhmNlVjew34Fv17LsXuDcVcTnnnGu61t7l5ZxzLkN4QnHOORcJTyjOOeci\n4QnFuRSwuANwLgaeUJxzzkXCE4pzzrlIeEJxzjkXCU8ozjnnIuEJxTnnXCQ8oTgXkd37Kqio8HlQ\nXNuV9Y+vdy5djviPadXLTy9Yw6slG5jxgyJycxRjVM6ljycU51Jg6659bN21j137KujS3v+bubbB\nu7ycc85FwhOKc865SHhCcc45FwlPKM61YjPeW8uQCU/z/trtcYfiXKNiTSiSLpC0RFKJpAl17P+d\npHnh631JWxL2VSTsm5LeyJ1LrcpK4/fPv8/kN1cB8NYHm2OOyLnGxXb5iaRc4E/Ap4DVwGxJU8zs\n3aoyZva9hPLfBk5IOMUuMxuZrnidS7UfPDKfwb078Z1zRzBz6Xp+//zSuENyrknivJ7xFKDEzJYD\nSJoMjAHeraf8VQRzzjuXMZK5A2XRR1vZunMf/3xrNQDfGnUoFRX+AHyXeRRM2x7DG0ufAy4ws+vC\n9auBU83shjrKDgZmAQPNrCLcVg7MA8qBiWb2RD3vMx4YD1BQUHDS5MmTmxVvWVkZXbp0adaxmcrr\n3DTjpu04YNtdozvRIa/htFLXcV3zYfve/evXHJ3POYPaNSuuxvj3nP1aWt9Ro0bNNbPCxsplyh1X\nVwKPVSWT0GAzK5U0DJgh6R0zW1b7QDObBEwCKCwstKKiomYFUFxcTHOPzVRe5yaa9vQBm8466yw6\nN3ZjYx3HJSYTgO79BrO6Q3u+fNrg5sXWAP+es1+66htnQikFBiWsDwy31eVK4FuJG8ysNPy5XFIx\nwfjKAQnFuWxQNZ7yqaMKKOjWIeZonKtbnFd5zQZGSBoqKZ8gaRxwtZakI4CewOsJ23pKah8u9wHO\noP6xF+diE3WHcnmlj6241iu2FoqZlUu6AZgO5AL3mtkiSbcBc8ysKrlcCUy2moM9RwJ/llRJkBQn\nJl4d5lxrVllpLN+wgz5d8lm5cWfc4TgXmVjHUMxsKjC11raba63fWsdxrwHHpjQ45yJQ10Uvf35p\nObdPe69Z5/PnFrvWzO+Udy7N3v7Qb1J02ckTinMptHvfgRNuqQXNjLc+3Myjc1a1ICLnUscTinMp\ndPIvnmfrrn2Rne+Gf7zNjx5bENn5nIuSJxTnUmzTjpo3lchHQlyW8oTinHMuEp5QnEux2ld6tWQM\nxbnWzBOKcyn2q2lL4g7BubTwhOJcik1b9HGNdW+huGzlCcU551wkPKE4l2ZRXOV11q9m8Nn/fS2C\naJyLTqY8vt45l2DVpl2s2rQr7jCcq8FbKM455yLhLRTn0uDJeaX06pzPiIO6xh2KcynjCcW5NLhx\n8jwA+nTJp2/X6CbIenJeKccN7MHQPp0jO6dzzeUJxbk02lC2lw1lexsvmKQbJ88jL0eU/PKiyM7p\nXHP5GIpzGa680rj/tZWs3LCDr943m3XbdscdkmujYk0oki6QtERSiaQJdewfJ2m9pHnh67qEfWMl\nLQ1fY9MbuXP7lVdUsq/iwMfUp9MtUxZR9JtiZry3jv8pXhZrLK7tiq3LS1Iu8CfgU8BqYLakKXVM\n5fuwmd1Q69hewC1AIcG03XPDY33mIpd25/3+JZav3xF3GNUqzShZt51D/QIAl2ZxtlBOAUrMbLmZ\n7QUmA2OSPPZ84Dkz2xQmkeeAC1IUp3MNak3JBODxt0oZfcdLvLhkXdyhuDYmzkH5AUDi1HOrgVPr\nKPdZSWcD7wPfM7NV9Rw7oK43kTQeGA9QUFBAcXFxs4ItKytr9rGZyuucmbbvKQdg2mvz0Zp2jZbP\nhjo3VVurc7rq29qv8noKeMjM9kj6OnA/8MmmnMDMJgGTAAoLC62oqKhZgRQXF9PcYzOV1zlJ055O\nSSwtNXz4MIrOHt5oOf+es1+66htnl1cpMChhfWC4rZqZbTSzPeHqX4CTkj3Wubau1jQszqVcnAll\nNjBC0lBJ+cCVwJTEApL6JaxeBiwOl6cD50nqKakncF64zTnnXExi6/Iys3JJNxAkglzgXjNbJOk2\nYI6ZTQG+I+kyoBzYBIwLj90k6WcESQngNjPblPZKONeKeQPFpVusYyhmNhWYWmvbzQnLPwF+Us+x\n9wL3pjRA5zJYVZdXRaVx98vLufq0wXRu39qHTV0m8zvlnWumveWV3PfqirjDqJeFbZRpCz9m4jPv\ncfu092KOyGU7/3PFuWa6++Xl/Hp6650vvqqFsqe8AoBtu/bFGI1rC7yF4lwz+S9o52ryhOJclqtq\nqUgtn3rYuYZ4QnEuS63YsIORtz3Lqs07ASKYyd65hvkYinNZ6om3SymvNB6dszrY4BnFpZi3UJxr\nptZ+n0debpBByiuDR+vLM4pLMU8ozjnnIuEJxblmslb+sKzd+4KWyf5B+RiDcW2CJxTn2ojtu/dx\n7K3TeX3ZxrhDcVnKE4pzWa6qHbWwdBvbd5fzxxlLY43HZS9PKM5luU079gJQumUX4I+1d6njCcW5\nLFdR6RnEpYcnFOeaKVPvPDeMXXsrWv1FBS7zeEJxrpkyNJ+wbVc5R948jT+8UBJ3KC7LxJpQJF0g\naYmkEkkT6tj/fUnvSlog6QVJgxP2VUiaF76m1D7WuVTLydCMsnFHMKv2E/N81mwXrdgevSIpF/gT\n8ClgNTBb0hQzezeh2NtAoZntlPQN4FfAF8J9u8xsZFqDdi5BTmbmk+o75r3Ly0UtzhbKKUCJmS03\ns73AZGBMYgEze9HMdoars4CBaY7RuXplagslMeztu/dR6YnFRSTOhDIAWJWwvjrcVp9rgWcS1jtI\nmiNplqTLUxGgc9lozdbdAGzfXc6xtz7LY+/v4/uPzOO1ZRtijsxluox42rCkLwOFwDkJmwebWamk\nYcAMSe+Y2bI6jh0PjAcoKCiguLi4WTGUlZU1+9hM5XVu2MqVe1MbTIpV3Z/yauletq4o5fG3Svnr\nBZ1jjio92tq/7XTVN86EUgoMSlgfGG6rQdJo4CbgHDPbU7XdzErDn8slFQMnAAckFDObBEwCKCws\ntKKiomYFW1xcTHOPzVRe54bN2bMElmfwlVIiuI1eOcGCaDPfd1v7t52u+jba5SWpk6T/kHR3uD5C\n0iURvPdsYISkoZLygSuBGldrSToB+DNwmZmtS9jeU1L7cLkPcAaQOJjvXMpZq3+AfXKqxlB8KMW1\nVDJjKH8F9gCnh+ulwM9b+sZmVg7cAEwHFgOPmNkiSbdJuiws9mugC/BorcuDjwTmSJoPvAhMrHV1\nmHMpl+m/gKviz/R6uNYjmS6v4Wb2BUlXAYSX8EZyeYuZTQWm1tp2c8Ly6HqOew04NooYnGvrPJ+4\nqCTTQtkrqSPhvztJwwlaLM65LOCP+nJRSaaFcgswDRgk6UGC8YpxqQzKOZc+nlBcVBpNKGb2nKS3\ngNMIrgu50cz8gnXX5mXL72EfQ3FRqTehSDqx1qY14c9DJB1iZm+lLiznXLpUhj8z9MZ/14o01EL5\nbfizA8FNhfMJWijHAXPYf9WXc21StvxlXz3nfLxhuCxQ76C8mY0ys1EELZMTzazQzE4iuIHQH1Pq\n2rzsuQ9l/8+de8vjDcZltGSu8jrczN6pWjGzhQT3gTjnskBiWvzp4wtji8NlvmSu8log6S/A38P1\nLwELUheScy4uH24KHu69r6KSikqjvNLYvGMvg3p1ijkylwmSSSjXAN8AbgzXXwL+N2UROedi0y43\n6LS44q7XmbdqC0f268biNdtYOfHimCNzmSCZy4Z3A78LX865KtkxhFKneau2ALB4zbYa27fv3scH\nG3dyzIDuTTpfeUUlFWa0z8uNLEbX+iTzcMgVkpbXfqUjOJf9zAwzo7yisvHCMXlyXikrNuw4YHsW\n55MDVM3u+J2H3uaSP77C7n0VTTr+wjtf5vCfTktFaK4VSabLqzBhuQNwBdArNeG4tqSi0jjz9hnk\n5YpVm3bx3188gXOPKGD3vgp6ds6PO7xqN06eR16OKPnlRdXbPtqyizdXbIoxqvSaMv8jFq/Zzhth\nnd/6cDPTFn7MrZceTU4ScyEvXVeW6hBdK5BMl9fGWpt+L2kucHNd5Z1Lxvrte7jjuferZw8EuOEf\nb1cvL7j1PLp1aNfoed5ZvZVr75/N1BvPok+X9pHHWfWXeXmt55NccdfrlG7ZFfn7tVY3Tp5XY/1L\nf3kDM/hm0aEc3L0DAGu27mL+qq1ccMzBcYToWoFkurxOTHgVSrqeDJnp0bVedzy3hIfe/LDe/U/O\n+4i/vroCgF17K/jxYwtYFV6BlGjSy8tZt30PryzdUD0DYZReWFw9DQ9LPt5evdyWkkldrPrelf2J\ndty9s7n+73Ob3B0GULJuO68v28iQCU/zg0fmRxWmS7NkEsNvE5bLgRXA51MTjmsrPth4YHJI9B9P\nBPdDbNtVzpZde3l4zirKK41zjzyIbz74FvddczJPvF1KZdhyeH3ZRr778Dz+du0pnDWib2Rx7kz4\n5Xj1PW/w5k11zqiQNV5fHvxST1Zi8liyNki4tzy5iGMGdOOumct54NpTGN63ywHHlawLus++dOpg\nAEbf8VL1vn++tZocwY/OP5zNO/fxvYfn8dD40+jesfEWq4tXMgnlWjOrMQgvaWiK4nFtwD/nrua1\nZbV7Uuv2u+ff33/cW6tZ9NFWAMb9dTYAxw8MrjZ6eel6AOav2oIQR/brSu8IusASr3Iq2+N3kdf2\n1Pw1/O759znhkB7V2x6es4qH5wTLzy5ay/iz989T//y7azm4ewcu+eMrANxUz42Uj85dzctLN/Dx\ntqBL9OWl67nkuP4pqoWLSjJ3yj+W5LYmk3SBpCWSSiRNqGN/e0kPh/vfkDQkYd9Pwu1LJJ0fRTwu\nPX7waPO7NN5L6HYCmL86SDC7wr+U3yndypfveYOv/20u81ZtYdWmnSxYvYV3Vm9l5vvrmfjMe016\nvxkJXV4791aw6KOtVPjz3qtVJfy3P9xS5/7bp73HmbfPqF6/7oE51cmkMVXJBILxtcN/+gxDJjzN\nK0s3MGTC0/xrwUctiNylQkNPGz4COBroLukzCbu6EVzt1SKScoE/AZ8CVgOzJU2pNZXvtcBmMztU\n0pXA7cAXJB1FMAf90UB/4HlJh5lZ0ztvk/D6so2Mm7YDpj3Nsl9eRG4SV7VkqpeXrufqe97k6e+c\nSXmlsXrzTv7vrVK+c+6IFp13b3kleTliTcIviSht3rkPgOmL1gIw54PNXP6nV2uU6dYhj227yznj\n0N78a/4aJn72WCRRWWn1XqlU+wm8F/8huV+Gbr/ECy9aYk95cGn5l+95AwiSzD2vrOCzJw7kwmMO\nZsr8jzh5SC+G9e1c436X2v9fLVue6tkKqb4PV9IY4HLgMmBKwq7twORwGt7mv7F0OnCrmZ0frv8E\nwMz+K6HM9LDM65LygI+BvsCExLKJ5Rp6z8LCQpszZ06TY21Kn3JdhvXtzPL1wX0MXdvnsT2h66R3\n53x27auo/k8w94PN1fu6ts9jYK9O1d0u+bk57E24XyM/L4expw9m8uxV7N5XQb/uHasfnVEj/t6d\nWFnPmMUZh/bm1ZLkup+S1bFdbnWLwTnXOrz/8wvJz0umU+pAkuaaWWFj5eptoZjZk8CTkk5v7Bd1\nMw0AViWsrwZOra+MmZVL2gr0DrfPqnXsgLreRNJ4YDxAQUEBxcXFUcTeJFXJBKiRTAA2hlcmLSyt\neUdyVdnEPvy9tW7+21teyd0vr6heryuZAPUmEyDyZAJ4MnGuFXpo6osM7pbaJxU01OX1b2b2K+CL\nkq6qvd/MvpPSyCJiZpOASRC0UIqKipp8jp4vPVvdpdKxXS5/vvokZi3fyMlDelHQrQOd8nMprzR2\n76vg6P7dmPPBZgb37sSydTvo27U9fbrks2NvBfm5OXTtkEfpll107ZDHqk07GdSzE3m5OazZuotD\nD+pCZSVsKNtDl/Z57C4Pjtmxp4LundqxY085f5xRQv/uHThzRB+6dWzHwJ4dmfvBZkYc1JV2uWLZ\n+jL69+jIph17yZHo0akdHfJy6dGpHW+u2MSgXp1on5fD5p17KejWAUksWLWFnBzxn0+9e8CjNhJd\ndOzBTH3nYz59wgCuPHkQm3bspW/X9vTsnM/arbsZ0qczHdvlsreikkozynaXs233Pl4r2cgHm3Zy\nytBe/Gb6EtZt39Os7zJKEz9zLABvrtjE2E8M4d5XV3DzJUfxyPRXOPrY4+jbtT2LPtrGD1sw3uPi\n8YnhvZm3ags791Yw/btnc+GdL3HLpUcz8Zn3GHfGED7ctJOX3lvD9aMOZ96qLRzVrxud2+eydG0Z\nR/fvxpK121m7bQ9Fh/dlyryPuOaMobyweC2nDuuFECs27qBzfi6DenViYelWRhR0ZdbyjZx4SE/6\nde/Ay0s3MLxvZw7q1oE/z1zGpcf3Jz8vhy7t87hx8jxuvfQoPtq6Gwm+UDiI7bvLmb7oYy445mD+\ntWANnzziIMorjPVluykc3IsNZXvYV2EM6tWRhaXbyBEc0qsTM95bx7lHFrByww5ycoJ7u5auLeOa\nM4eyfvseDu7WgftfX8lnTxzIEzNmMfayc1P+2TfU5XWpmT0laWxd+83s/ha9cQZ1eVVUGsP/fSp3\nXjmSMSPrbAhljS079zLz/fWMGTmA4uJimpOAG7Jpx17+e0YJ9766ovHCLTTp6pNYuq6ML516CJt3\n7mNI7078a8EaBvXqxMhBPeo8pnadf/jofB6bu7p6/UfnH86vpy9JdehZ5TufPJQ/zCiJ/LyLb7uA\nd0q3csrQpj+4IxX/tluzltY3ii6vp8KfLUocDZgNjAgvQS4lGGT/Yq0yU4CxwOvA54AZZmaSpgD/\nkHQHwaD8CODNFMVJbo6474LOFGV5MgHo0Sk/pUmzV+d8ThvWK6UJZe5PR7N0XRmnDevNeUcH23p0\nCh7lcunxTbv0tLLWH1zfGnUoS9du54l5foVRMqouYqlKKPeOK+SO596vs4u3qTrm5zYrmbjUqXeE\nRtJTkqbU92rpG5tZOXADMB1YDDxiZosk3SbpsrDYPUBvSSXA99nfMlkEPAK8C0wDvpWqK7xc9Drm\nR9OPu/A/z+c3VxxfvX7VKYdw/MDu9O7SntOG9Y7kPY6r46m6v7/yBP56zcmRnL81uuDoaB6dcu2Z\nQ6uvsBrQoyMAIw7qyv988SQ+ecRB3PXlk+o99pzDDrw5tUO7HI7u341ZPzmXZ248K5IYXbQaurHx\nN6l+czObCkytte3mhOXdBA+jrOvYXwC/SGmALiVyal+L24h/fuMTlO0pZ1DPjnRol8vqzbvo27U9\nXdrn8bmTBtK9YzsqzTjvqILI53mv73LidjnNu1omE/zs8mOYtujjJh1z7hEH8cJ7++/ZeebGszi8\noGv1etW9O3m5ol/3jtw7LkjIVfOs7KuoZMRNz1SXP3ZAd9Zv38PoIw+qbt0svPX88Bw51c8Pc61L\nQ11eM6uWJeUDRxA8sXuJmUX/0CTXZlT9tdqYcZ8Ywr9dcDid8mv+M+1f6/hPHVVQvdzEXNVsebnZ\ney9Sfu7+ZHl0/24s+qju7ql/fO1UDu7WgWHho1W2795HjsS23fvo173md1ReGVyhmFdPIm6Xm8Ok\nq0/i+EE9KFlXxqlDe/HD8w8HqE4oebnZm8SzRaOPXpF0MXAXsAwQMFTS183smYaPdK5uQ/p05tUJ\nn+SMiTMaLJefl3NAMkm3YX32P4fq4fGnVS+3y+KE0r1TO/541QnsKa/k/KMLeKd0K4N7d67+vr52\n1lDOPqwvnxjep8ZxXcOnQ3duf+B39oWTB/GnF5fRtUP93+d5YVdbQbearY9ffvpYhvT2KYgzQbIP\nhxxlZiUAkoYDTwOeUFyzNdRKOWVILwb37sT15wxPY0R1O3NEH5773tkcelAXlND8yfanryRevFCV\nOM4/uoBGtBW5AAASa0lEQVTPnDiQ85sxxvLD8w7nu6MPq55iuCm+eOohTT7GxSOZhLK9KpmElhPc\nLe9ci3xv9GE1Hv4I8Ma/n0uPTu1a1VSxIxLGAqo05xHtme7PVzd61Wi9JGV1q84FkkkocyRNJbiq\nyggGyWdXPd/LzP4vhfG5LHbj6BGcPrw3n//z61w+sj9jThhwQHdHa9Wljm4d59q6ZP5XdADWAueE\n6+uBjsClBAnGE4prtlOG9mL6d8/msIKaXUqt3QmH9OR/vnQi33zwrbhDidT/ffMTcYfgMlgyUwBf\nk45AXNt1+MEHdillgjNqDUpng+MH1v0EAeeSkcxVXkOBbwNDEsub2WX1HeNcm5A5DaqkZfHMDC4N\nkunyeoLgjvWngMpGyjrXZmRQD13SMqnb0bU+ySSU3Wb2h5RH4pxzLqMlk1DulHQL8CxQ/dxxM8uu\n0Ujnmsj/lneupmQSyrHA1cAn2d/lZeG6c22Wdw85V1MyCeUKYJg/v8u5mjydOFdTMs9BWAj4tYTO\nOecalEwLpQfwnqTZ7B9DMTMbk7qwnGv9vMfLuZqSSSi3JCwLOItgdkXn2jR5p5dzNTTa5RXOi7IN\nuAS4j2Aw/q6WvKmkXpKek7Q0/NmzjjIjJb0uaZGkBZK+kLDvPkkrJM0LXyNbEo9zbd315wznlCE+\nna5rmXpbKJIOA64KXxuAhwGZ2agI3ncC8IKZTZQ0IVz/ca0yO4GvmNlSSf2BuZKmm9mWcP+PzOyx\nCGJxrlmyqctrwoVHxB2CywINtVDeI2iNXGJmZ5rZH4Gontk9Brg/XL4fuLx2ATN738yWhssfAeuA\nAyeads451yrI6pmEW9LlBGMlZwDTgMnAX8xsaIvfVNpiZj3CZQGbq9brKX8KQeI52swqJd0HnE5w\nkcALwAQz21PPseOB8QAFBQUnTZ48uVkxl5WV0aVLl8YLZhGvc8P2VRpfe3ZniiNKre+e2J6tO3Zz\nzlD/nrNZS+s7atSouWbW6IQ49SaU6gJSZ4IWxVUELZYHgMfN7NlGjnseqGtqt5uA+xMTiKTNZnbA\nOEq4rx9QDIw1s1kJ2z4G8oFJwDIzu63BigCFhYU2Z86cxorVqbi4mKKiomYdm6m8zg3bU17B4T+d\nltqAUmzlxIv9e24DWlpfSUkllGQeX78D+Afwj3Dw/AqC8Y4GE4qZjW4guLWS+pnZmjA5rKunXDeC\n6YZvqkom4bnXhIt7JP0V+GFj9XAuan6Vl3M1NWmCZzPbbGaTzOzcFr7vFGBsuDwWeLJ2AUn5wOPA\nA7UH38MkVNVddjnBzZfOpVUmD8of2a8bR/brFncYLsvENY/pROARSdcCHwCfB5BUCFxvZteF284G\neksaFx43zszmAQ9K6ktwX8w84Po0x+9cRnvmxrPiDsFloVgSipltBA5o5ZjZHOC6cPnvwN/rOd4f\nTOlil8ENFOdSokldXs65/fxpw87V5AnFOedcJDyhONdM3j5xriZPKM41k/d4OVeTJxTnmqm1j6FU\nhTfuE0NijcO1HXFdNuycS4OVEy8G4L7XVgLwr2+fSZf2/t/epYb/y3IuS+XlHNiCOmZA9xgicW2F\nd3k5l6VyWnmXnMs+nlCcy1K5dbRQnEslTyjOZamvntHimSacaxIfQ3EuC62ceDGNTU3hXNS8heJc\nC1xx0sC4Q6hXa7+s2WUfb6E41wK/vuJ4Pti4kzdXboo7lAb99OIjOXFwnXPYORcZTyjOtZDR+ruW\nrjtrWNwhuDbAu7yca6HK1p9PnEsLTyjOtVClD347B8SUUCT1kvScpKXhzzo7dyVVSJoXvqYkbB8q\n6Q1JJZIeDqcLdi4Wnk+cC8TVQpkAvGBmI4AXwvW67DKzkeHrsoTttwO/M7NDgc3AtakN17n6eT5x\nLhBXQhkD3B8u3w9cnuyBCq6F/CTwWHOOdy5qrel+j59ceAT/+vaZcYfh2ijF8Z9B0hYz6xEuC9hc\ntV6rXDkwDygHJprZE5L6ALPC1gmSBgHPmNkx9bzXeGA8QEFBwUmTJ09uVsxlZWV06dKlWcdmKq9z\ncm55bRcfbKtMUUTJuXBoO/p1FmcPbNfkY/17zn4tre+oUaPmmllhY+VSdtmwpOeBg+vYdVPiipmZ\npPqy2mAzK5U0DJgh6R1ga1PiMLNJwCSAwsJCKyoqasrh1YqLi2nusZnK65ycLgtehm3bUhNQkv73\n6+c1+1j/nrNfuuqbsi4vMxttZsfU8XoSWCupH0D4c1095ygNfy4HioETgI1AD0lVyXAgUJqqejjX\nmJsvOYrhfTs3WObNfz83TdE4F5+4xlCmAGPD5bHAk7ULSOopqX243Ac4A3jXgj66F4HPNXS8c+ly\n6rDevPCDogbLSPKxDZf14kooE4FPSVoKjA7XkVQo6S9hmSOBOZLmEySQiWb2brjvx8D3JZUAvYF7\n0hq9c00k+eRWLvvF8ugVM9sIHNAHYGZzgOvC5deAY+s5fjlwSipjdC5KidPu9u/egY+27o7kvM9/\n/xz2VcR7QYBzVfxZXs6l2NmH9aVDu1wASn5xIR9s2sm5v53ZonP+7PJj6Nsln0MPajtXKrnWzxOK\nc2mUl5sTydS8g3t14uzD+kYQkXPR8Wd5OZdiqZiVxKc6ca2RJxTnnHOR8ITiXJpF8XQKpaTd41zL\neEJxzjkXCU8ozqXY9ecMjzsE59LCE4pzKXb68N6Rn7NHp6Y/BNK5VPPLhp1Ls5aMoFw+sj9jThjg\nd927VslbKM7FZFifzpT84kKGNfJgyUTt83IZdfhBKYzKuebzFopzaTakd2c+d9JArj1zKHm5OT7l\no8sanlCcS7PcHPGbK44/YHvvzvls3LE3hoici4Z3eTmXQp85YUDSZf9+3anMv6X5E2U5FzdPKM7F\n7MpTBgHQv0dHunf0q7dc5vKE4lzMvnbWMFb810V1JpMhvTvVWPdneLnWLJaEIqmXpOckLQ1/9qyj\nzChJ8xJeuyVdHu67T9KKhH0j018L56IhCdWRKd659TymffdsLjmuH7/67HEAXHxcv3SH51zS4hqU\nnwC8YGYTJU0I13+cWMDMXgRGQpCAgBLg2YQiPzKzx9IUr3Np17VD0GL57y+eCMDnTx4UZzjONSqu\nLq8xwP3h8v3A5Y2U/xzwjJntTGlUzjnnmi2uFkqBma0Jlz8GChopfyVwR61tv5B0M/ACMMHM9kQc\no3OxePT608nL8cESl3kUxaO06zyx9DxwcB27bgLuN7MeCWU3m9kB4yjhvn7AAqC/me1L2PYxkA9M\nApaZ2W31HD8eGA9QUFBw0uTJk5tVn7KyMrp0aVvTrXqdm2bctB0HbPtE/zzGH9e+pWGllH/P2a+l\n9R01atRcMytsrFzKWihmNrq+fZLWSupnZmvC5LCugVN9Hni8KpmE565q3eyR9Ffghw3EMYkg6VBY\nWGhFRUVNqMV+xcXFNPfYTOV1bqJpTx+w6eCCAoqKWvc1I/49Z7901TeuMZQpwNhweSzwZANlrwIe\nStwQJiEUXBpzObAwBTE655xrgrgSykTgU5KWAqPDdSQVSvpLVSFJQ4BBwMxaxz8o6R3gHaAP8PM0\nxOxc0/lQiGtDYhmUN7ONwLl1bJ8DXJewvhI44NkVZvbJVMbnnHOu6fxOeedSyOd+d22JJxTnnHOR\n8ITiXAr17pIfdwjOpY3Ph+Jcivzy08fymROTf3y9c5nOE4pzKfLFUw+JOwTn0sq7vJxzzkXCE4pz\nEbltzNFccdLAuMNwLjbe5eVcRL5y+hAAHp27Ot5AnIuJt1Ccc85FwhOKc865SHhCcc45FwlPKM45\n5yLhCcU551wkPKE455yLhCcU55xzkfCE4pxzLhKxJBRJV0haJKlSUr0T30u6QNISSSWSJiRsHyrp\njXD7w5L8ka7OORezuFooC4HPAC/VV0BSLvAn4ELgKOAqSUeFu28HfmdmhwKbgWtTG65zzrnGxJJQ\nzGyxmS1ppNgpQImZLTezvcBkYIwkAZ8EHgvL3Q9cnrponXPOJaM1P8trALAqYX01cCrQG9hiZuUJ\n2+uddELSeGA8QEFBAcXFxc0KpqysrNnHZiqvc/OMPSqfwd1yMuaz8+85+6WrvilLKJKeBw6uY9dN\nZvZkqt63NjObBEwCKCwstKKiomadp7i4mOYem6m8zs3TsqPTz7/n7Jeu+qYsoZjZ6BaeohQYlLA+\nMNy2EeghKS9spVRtd845F6PWfNnwbGBEeEVXPnAlMMXMDHgR+FxYbiyQthaPc865usV12fCnJa0G\nTgeeljQ93N5f0lSAsPVxAzAdWAw8YmaLwlP8GPi+pBKCMZV70l0H55xzNcUyKG9mjwOP17H9I+Ci\nhPWpwNQ6yi0nuArMOedcK9Gau7ycc85lEE8ozjnnIuEJxTnnXCQ8oTjnnIuEgqtw2wZJ64EPmnl4\nH2BDhOFkAq9z2+B1zn4tre9gM+vbWKE2lVBaQtIcM6v3ycjZyOvcNnids1+66utdXs455yLhCcU5\n51wkPKEkb1LcAcTA69w2eJ2zX1rq62MozjnnIuEtFOecc5HwhOKccy4SnlBqkXSBpCWSSiRNqGN/\ne0kPh/vfkDQk/VFGK4k6f1/Su5IWSHpB0uA44oxSY3VOKPdZSSYpoy8xTaa+kj4ffs+LJP0j3TFG\nLYl/14dIelHS2+G/7YvqOk8mkXSvpHWSFtazX5L+EH4mCySdGGkAZuav8AXkAsuAYUA+MB84qlaZ\nbwJ3hctXAg/HHXca6jwK6BQuf6Mt1Dks1xV4CZgFFMYdd4q/4xHA20DPcP2guONOQ50nAd8Il48C\nVsYddwT1Phs4EVhYz/6LgGcAAacBb0T5/t5CqekUoMTMlpvZXmAyMKZWmTHA/eHyY8C5kpTGGKPW\naJ3N7EUz2xmuziKYJTOTJfM9A/wMuB3Ync7gUiCZ+n4N+JOZbQYws3VpjjFqydTZgG7hcnfgozTG\nlxJm9hKwqYEiY4AHLDCLYPbbflG9vyeUmgYAqxLWV4fb6ixjwSRgWwkm+cpUydQ50bUEf+Fkskbr\nHHYFDDKzp9MZWIok8x0fBhwm6VVJsyRdkLboUiOZOt8KfDmc7G8q8O30hBarpv5/b5JYJthymUnS\nl4FC4Jy4Y0klSTnAHcC4mENJpzyCbq8ighboS5KONbMtsUaVWlcB95nZbyWdDvxN0jFmVhl3YJnK\nWyg1lQKDEtYHhtvqLCMpj6CpvDEt0aVGMnVG0mjgJuAyM9uTpthSpbE6dwWOAYolrSToa56SwQPz\nyXzHq4EpZrbPzFYA7xMkmEyVTJ2vBR4BMLPXgQ4ED1HMZkn9f28uTyg1zQZGSBoqKZ9g0H1KrTJT\ngLHh8ueAGRaOdmWoRuss6QTgzwTJJNP71qGROpvZVjPrY2ZDzGwIwbjRZWY2J55wWyyZf9dPELRO\nkNSHoAtseTqDjFgydf4QOBdA0pEECWV9WqNMvynAV8KrvU4DtprZmqhO7l1eCcysXNINwHSCq0Tu\nNbNFkm4D5pjZFOAegqZxCcHg15XxRdxySdb510AX4NHw+oMPzeyy2IJuoSTrnDWSrO904DxJ7wIV\nwI/MLGNb3knW+QfA3ZK+RzBAPy7D/zhE0kMEfxj0CceGbgHaAZjZXQRjRRcBJcBO4JpI3z/DPz/n\nnHOthHd5Oeeci4QnFOecc5HwhOKccy4SnlCcc85FwhOKc865SHhCca6ZJPWWNC98fSypNGH9tQjf\n53JJNzew/1hJ90X1fs41l1827FwEJN0KlJnZb1Jw7tcIbqzc0ECZ54GvmtmHUb+/c8nyFopzKSCp\nLPxZJGmmpCclLZc0UdKXJL0p6R1Jw8NyfSX9U9Ls8HVGuP0wYE9VMpF0haSFkuZLeinhLZ8iw2+y\ndZnPE4pzqXc8cD1wJHA1cJiZnQL8hf1PuL0T+J2ZnQx8NtwHcAbwVsK5bgbON7PjgcSnFcwBzkpZ\nDZxLgj96xbnUm131vCRJy4Bnw+3vEExeBjAaOCphap1ukroA/aj5fKlXgfskPQL8X8L2dUD/1ITv\nXHI8oTiXeolPZ65MWK9k///BHOA0M6sxmZekXQRPtAbAzK6XdCpwMTBX0knhM7c6ALtSFL9zSfEu\nL+dah2dJmOBJ0shwcTFwaML24Wb2hpndTNByqXoU+WFAnfOIO5cunlCcax2+AxRKWhA+8ff6cPtL\nwAkJ00z/OhzMXwi8RjBXOgRdZ9kwu6TLYH7ZsHOtnKQ7gafM7Pl69rcHZgJnhtNSOxcLb6E41/r9\nEujUwP5DgAmeTFzcvIXinHMuEt5Ccc45FwlPKM455yLhCcU551wkPKE455yLhCcU55xzkfh/ABRj\nk9TD51MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10cb09a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_file = \"{}/bed/0a7c2a8d_nohash_0.wav\".format(DATA_DIR)\n",
    "\n",
    "with wave.open(test_file, 'rb') as f:\n",
    "    params = f.getparams()\n",
    "    nchannels, sampwidth, framerate, nframes = params[:4]\n",
    "#     print(nchannels, sampwidth, framerate, nframes) # 1 2 16000 16000\n",
    "    strData = f.readframes(nframes)#读取音频，字符串格式\n",
    "#     print(type(strData)) # <class 'bytes'>\n",
    "#     print(strData[:20]) # b'\\xff\\xff\\xea\\xff\\xe3\\xff\\xef\\xff\\xf6\n",
    "\n",
    "waveData = np.fromstring(strData, dtype=np.int16)#将字符串转化为int\n",
    "# print(type(waveData)) # <class 'numpy.ndarray'>\n",
    "# print(waveData[:20]) # [ -1 -22 -29 -17 -10 -14 -19 -18 -18 -21 -11 8]\n",
    "\n",
    "waveData_norm = waveData * 1.0 / (max(abs(waveData)))#wave幅值归一化\n",
    "#     print(type(waveData_norm)) #<class 'numpy.ndarray'>\n",
    "#     print(waveData_norm[:20]) #[-0.00010379 -0.00228334 -0.00300986 -0.0017644]\n",
    "# print(waveData_norm.shape) # (16000,)\n",
    "\n",
    "time = np.arange(0, nframes)*(1.0 / framerate)\n",
    "plt.plot(time, waveData_norm)\n",
    "plt.xlabel(\"Time(s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.title(\"Single channel wavedata\")\n",
    "plt.grid('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wave_norm(file):\n",
    "    with wave.open(file, 'rb') as f:\n",
    "        params = f.getparams()\n",
    "        nchannels, sampwidth, framerate, nframes = params[:4]\n",
    "        data = f.readframes(nframes)\n",
    "    data = np.fromstring(data, dtype=np.int16)\n",
    "    data = data * 1.0 / max(abs(data))\n",
    "    return data\n",
    "\n",
    "def save_wave(data, file='./save.wav'):\n",
    "    with wave.open(file, 'wb') as outwave:\n",
    "        nchannels = 1\n",
    "        sampwidth = 2\n",
    "        framerate = 16000\n",
    "        nframes = data.shape[0]\n",
    "        comptype = \"NONE\"\n",
    "        compname = \"not compressed\"\n",
    "        outwave.setparams((nchannels, sampwidth, framerate, nframes, comptype, compname))\n",
    "\n",
    "        for v in data:\n",
    "            outwave.writeframes(struct.pack('h', int(v * 64000 / 2)))#outData:16位，-32767~32767，注意不要溢出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_FRAME = 16000\n",
    "LABELS = ['five', 'happy', 'one', 'house', 'tree', 'bed', 'marvin', 'dog', 'left', 'stop', 'sheila', \n",
    "          'four', 'zero', 'cat', 'three', 'two', 'bird', 'yes', 'wow', 'seven', 'on', 'down', 'nine', \n",
    "          'right', 'up', 'no', 'eight', 'six', 'off', 'go']\n",
    "N_CLASS = len(LABELS)\n",
    "\n",
    "file_glob, noise_glob = [], []\n",
    "for f in glob('{}/*/*.wav'.format(DATA_DIR)):\n",
    "    # bug, this wav has no sound\n",
    "    if f.find('bird/3e7124ba_nohash_0.wav') != -1:\n",
    "        continue\n",
    "    \n",
    "    if f.find('/_background_noise_/') == -1:\n",
    "        file_glob.append(f)\n",
    "    else:\n",
    "        noise_glob.append(f)\n",
    "\n",
    "noize_data = [get_wave_norm(f) for f in noise_glob]\n",
    "noize_data = np.concatenate(noize_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_noise(w):\n",
    "    idx = random.randint(0, noize_data.shape[0] - w.shape[0])\n",
    "    w = w + noize_data[idx:idx+w.shape[0]] / 5.0\n",
    "    w = w * 1.0 / (max(abs(w)))\n",
    "    return w\n",
    "\n",
    "def get_wave(file):\n",
    "    data = get_wave_norm(file)\n",
    "    \n",
    "#     if random.random() >= 0.5:\n",
    "#         data = add_noise(data)\n",
    "\n",
    "    wave_data = np.zeros((MAX_FRAME, ))\n",
    "    wave_len = min(MAX_FRAME, data.shape[0])\n",
    "    wave_data[:wave_len] = data[:wave_len]\n",
    "    \n",
    "    return np.expand_dims(wave_data, axis=1)\n",
    "\n",
    "# test\n",
    "# x = get_wave('{}/bed/0a7c2a8d_nohash_0.wav'.format(DATA_DIR))\n",
    "# save_wave(x[:,0], 'mix.wav')\n",
    "\n",
    "def gen(batch_size=32, verbose=False):\n",
    "    while True:\n",
    "        X = np.zeros((batch_size, MAX_FRAME, 1), dtype=np.float32)\n",
    "        y = np.zeros((batch_size, N_CLASS), dtype=np.uint8)\n",
    "        \n",
    "        files = np.random.choice(file_glob, batch_size)\n",
    "        if verbose:\n",
    "            print(files)\n",
    "            \n",
    "        for i in range(batch_size):\n",
    "            f = files[i]\n",
    "            X[i] = get_wave(f)\n",
    "            \n",
    "            label = f[len(DATA_DIR) + 1:]\n",
    "            label = label[:label.index('/')]\n",
    "            label_idx = LABELS.index(label)\n",
    "            y[i, label_idx] = 1\n",
    "            \n",
    "        yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class AudioEvaluator(Callback):\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         print('*******')\n",
    "#         X_test, y_test = next(gen(1, verbose=True)) # (1, 128, 128, 1)\n",
    "#         y_pred = self.model.predict(X_test) # (1, 128, 128, AB_PAIRS)\n",
    "#         print(\"predict:{}\\n*******\".format(LABELS[np.argmax(y_pred[0])]))\n",
    "\n",
    "# evaluator = AudioEvaluator()\n",
    "\n",
    "def do_train(m):\n",
    "    global RUN\n",
    "    RUN += 1\n",
    "    print(\"RUN {}\".format(RUN))\n",
    "\n",
    "    LOG_DIR = '/output/training_logs/run-{}'.format(RUN)\n",
    "    LOG_FILE_PATH = LOG_DIR + '/checkpoint-{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "\n",
    "    # tensorboard = TensorBoard(log_dir=LOG_DIR, histogram_freq=1, write_grads=False, write_graph=False)\n",
    "    tensorboard = TensorBoard(log_dir=LOG_DIR, write_graph=False)\n",
    "    checkpoint = ModelCheckpoint(filepath=LOG_FILE_PATH, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "\n",
    "    history = m.fit_generator(generator=gen(32), steps_per_epoch=256,\n",
    "                                  validation_data=gen(32), validation_steps=16,\n",
    "                                  epochs=10000, verbose=1, callbacks=[tensorboard, checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_conv_model():\n",
    "    m = Sequential()\n",
    "    m.add(Conv1D(filters=64, kernel_size=2, strides=2, padding='valid', activation='relu', input_shape=(MAX_FRAME,1)))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(Conv1D(filters=64, kernel_size=2, strides=2, padding='valid', activation='relu'))\n",
    "    m.add(BatchNormalization())\n",
    "#     m.add(Conv1D(filters=64, kernel_size=2, strides=2, padding='valid', activation='relu'))\n",
    "#     m.add(BatchNormalization())\n",
    "    m.add(Flatten())\n",
    "#     m.add(GlobalAveragePooling1D())\n",
    "    m.add(Dropout(0.3))\n",
    "    m.add(Dense(N_CLASS, activation='softmax'))\n",
    "    m.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_22 (Conv1D)           (None, 8000, 64)          192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 4000, 64)          8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 256000)            0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 256000)            0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 30)                7680030   \n",
      "=================================================================\n",
      "Total params: 7,688,990\n",
      "Trainable params: 7,688,734\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "RUN 12\n",
      "Epoch 1/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.2373 - acc: 0.1605Epoch 00000: val_loss improved from inf to 3.46807, saving model to /output/training_logs/run-12/checkpoint-00-3.4681.hdf5\n",
      "*******\n",
      "['./five/5828dfa2_nohash_1.wav']\n",
      "predict:up\n",
      "*******\n",
      "256/256 [==============================] - 25s - loss: 3.2347 - acc: 0.1609 - val_loss: 3.4681 - val_acc: 0.0547\n",
      "Epoch 2/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.6833 - acc: 0.2915Epoch 00001: val_loss did not improve\n",
      "*******\n",
      "['./tree/b5935410_nohash_1.wav']\n",
      "predict:two\n",
      "*******\n",
      "256/256 [==============================] - 23s - loss: 2.6834 - acc: 0.2919 - val_loss: 3.7236 - val_acc: 0.0488\n",
      "Epoch 3/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.4733 - acc: 0.3501Epoch 00002: val_loss improved from 3.46807 to 2.47949, saving model to /output/training_logs/run-12/checkpoint-02-2.4795.hdf5\n",
      "*******\n",
      "['./on/e54a0f16_nohash_3.wav']\n",
      "predict:one\n",
      "*******\n",
      "256/256 [==============================] - 23s - loss: 2.4747 - acc: 0.3501 - val_loss: 2.4795 - val_acc: 0.3262\n",
      "Epoch 4/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.3175 - acc: 0.4009Epoch 00003: val_loss improved from 2.47949 to 2.16091, saving model to /output/training_logs/run-12/checkpoint-03-2.1609.hdf5\n",
      "*******\n",
      "['./tree/51055bda_nohash_0.wav']\n",
      "predict:tree\n",
      "*******\n",
      "256/256 [==============================] - 23s - loss: 2.3176 - acc: 0.4005 - val_loss: 2.1609 - val_acc: 0.4492\n",
      "Epoch 5/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.1433 - acc: 0.4534Epoch 00004: val_loss improved from 2.16091 to 2.13564, saving model to /output/training_logs/run-12/checkpoint-04-2.1356.hdf5\n",
      "*******\n",
      "['./left/7dc50b88_nohash_0.wav']\n",
      "predict:left\n",
      "*******\n",
      "256/256 [==============================] - 23s - loss: 2.1440 - acc: 0.4531 - val_loss: 2.1356 - val_acc: 0.4375\n",
      "Epoch 6/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.0385 - acc: 0.4917Epoch 00005: val_loss improved from 2.13564 to 1.87675, saving model to /output/training_logs/run-12/checkpoint-05-1.8768.hdf5\n",
      "*******\n",
      "['./on/190821dc_nohash_2.wav']\n",
      "predict:eight\n",
      "*******\n",
      "256/256 [==============================] - 23s - loss: 2.0376 - acc: 0.4917 - val_loss: 1.8768 - val_acc: 0.4961\n",
      "Epoch 7/10000\n",
      " 27/256 [==>...........................] - ETA: 14s - loss: 2.0582 - acc: 0.5069"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-839ed5d4c2b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_conv_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdo_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-b9b840a1cc11>\u001b[0m in \u001b[0;36mdo_train\u001b[0;34m(m)\u001b[0m\n\u001b[1;32m     29\u001b[0m     history = m.fit_generator(generator=gen(32), steps_per_epoch=256,\n\u001b[1;32m     30\u001b[0m                                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                                   epochs=10000, verbose=1, callbacks=[tensorboard, checkpoint, early_stopping, evaluator])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1108\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1888\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1889\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1890\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1631\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1633\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1634\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2227\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2229\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "m = get_conv_model()\n",
    "m.summary()\n",
    "do_train(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN & RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_cnn_rnn_model():\n",
    "    ipt = Input(shape=(MAX_FRAME, 1))\n",
    "    x = ipt\n",
    "    x = Conv1D(filters=64, kernel_size=50, strides=50, padding='valid', activation='relu')(x)\n",
    "    x = Conv1D(filters=64, kernel_size=50, strides=50, padding='valid', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # LSTM\n",
    "    x = LSTM(32)(x)\n",
    "    \n",
    "    # double\n",
    "#     x = LSTM(32, return_sequences=True)(x)\n",
    "#     x = LSTM(32, return_sequences=False)(x)\n",
    "    \n",
    "    # 正反\n",
    "#     r   = LSTM(32, return_sequences=True, go_backwards=False)(x)\n",
    "#     r_b = LSTM(32, return_sequences=True, go_backwards=True)(x)\n",
    "#     x = concatenate([r, r_b])\n",
    "#     x = Flatten()(x)\n",
    "\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(N_CLASS, activation='softmax')(x)\n",
    "    \n",
    "    m = Model(inputs=[ipt], outputs=[x])\n",
    "    m.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "    \n",
    "    return m\n",
    "\n",
    "# get_cnn_rnn_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 16000, 1)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 320, 64)           3264      \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 6, 64)             204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 6, 64)             256       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                990       \n",
      "=================================================================\n",
      "Total params: 221,790\n",
      "Trainable params: 221,662\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "RUN 2\n",
      "Epoch 1/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.1525 - acc: 0.1005Epoch 00000: val_loss improved from inf to 3.06295, saving model to /output/training_logs/run-2/checkpoint-00-3.0629.hdf5\n",
      "256/256 [==============================] - 20s - loss: 3.1521 - acc: 0.1006 - val_loss: 3.0629 - val_acc: 0.1055\n",
      "Epoch 2/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.7591 - acc: 0.1756Epoch 00001: val_loss improved from 3.06295 to 2.58687, saving model to /output/training_logs/run-2/checkpoint-01-2.5869.hdf5\n",
      "256/256 [==============================] - 20s - loss: 2.7582 - acc: 0.1755 - val_loss: 2.5869 - val_acc: 0.2188\n",
      "Epoch 3/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.5560 - acc: 0.2327Epoch 00002: val_loss improved from 2.58687 to 2.31983, saving model to /output/training_logs/run-2/checkpoint-02-2.3198.hdf5\n",
      "256/256 [==============================] - 20s - loss: 2.5550 - acc: 0.2325 - val_loss: 2.3198 - val_acc: 0.2871\n",
      "Epoch 4/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.3744 - acc: 0.2857Epoch 00003: val_loss improved from 2.31983 to 2.14756, saving model to /output/training_logs/run-2/checkpoint-03-2.1476.hdf5\n",
      "256/256 [==============================] - 20s - loss: 2.3737 - acc: 0.2860 - val_loss: 2.1476 - val_acc: 0.3652\n",
      "Epoch 5/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.1976 - acc: 0.3349Epoch 00004: val_loss improved from 2.14756 to 2.09692, saving model to /output/training_logs/run-2/checkpoint-04-2.0969.hdf5\n",
      "256/256 [==============================] - 20s - loss: 2.1980 - acc: 0.3351 - val_loss: 2.0969 - val_acc: 0.3789\n",
      "Epoch 6/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.0676 - acc: 0.3746Epoch 00005: val_loss improved from 2.09692 to 1.94325, saving model to /output/training_logs/run-2/checkpoint-05-1.9432.hdf5\n",
      "256/256 [==============================] - 20s - loss: 2.0670 - acc: 0.3749 - val_loss: 1.9432 - val_acc: 0.4102\n",
      "Epoch 7/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.9416 - acc: 0.4154Epoch 00006: val_loss improved from 1.94325 to 1.86110, saving model to /output/training_logs/run-2/checkpoint-06-1.8611.hdf5\n",
      "256/256 [==============================] - 20s - loss: 1.9409 - acc: 0.4160 - val_loss: 1.8611 - val_acc: 0.4648\n",
      "Epoch 8/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.8504 - acc: 0.4456Epoch 00007: val_loss improved from 1.86110 to 1.79853, saving model to /output/training_logs/run-2/checkpoint-07-1.7985.hdf5\n",
      "256/256 [==============================] - 20s - loss: 1.8501 - acc: 0.4456 - val_loss: 1.7985 - val_acc: 0.4707\n",
      "Epoch 9/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.7899 - acc: 0.4667Epoch 00008: val_loss improved from 1.79853 to 1.63095, saving model to /output/training_logs/run-2/checkpoint-08-1.6309.hdf5\n",
      "256/256 [==============================] - 20s - loss: 1.7897 - acc: 0.4667 - val_loss: 1.6309 - val_acc: 0.5332\n",
      "Epoch 10/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.7166 - acc: 0.4915Epoch 00009: val_loss improved from 1.63095 to 1.52123, saving model to /output/training_logs/run-2/checkpoint-09-1.5212.hdf5\n",
      "256/256 [==============================] - 20s - loss: 1.7188 - acc: 0.4902 - val_loss: 1.5212 - val_acc: 0.5723\n",
      "Epoch 11/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.6267 - acc: 0.5201Epoch 00010: val_loss improved from 1.52123 to 1.48726, saving model to /output/training_logs/run-2/checkpoint-10-1.4873.hdf5\n",
      "256/256 [==============================] - 20s - loss: 1.6263 - acc: 0.5203 - val_loss: 1.4873 - val_acc: 0.5430\n",
      "Epoch 12/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.5699 - acc: 0.5243Epoch 00011: val_loss did not improve\n",
      "256/256 [==============================] - 20s - loss: 1.5690 - acc: 0.5248 - val_loss: 1.5461 - val_acc: 0.5586\n",
      "Epoch 13/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.5329 - acc: 0.5461Epoch 00012: val_loss improved from 1.48726 to 1.40894, saving model to /output/training_logs/run-2/checkpoint-12-1.4089.hdf5\n",
      "256/256 [==============================] - 20s - loss: 1.5320 - acc: 0.5468 - val_loss: 1.4089 - val_acc: 0.5762\n",
      "Epoch 14/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.4978 - acc: 0.5620Epoch 00013: val_loss improved from 1.40894 to 1.25012, saving model to /output/training_logs/run-2/checkpoint-13-1.2501.hdf5\n",
      "256/256 [==============================] - 19s - loss: 1.4962 - acc: 0.5627 - val_loss: 1.2501 - val_acc: 0.6484\n",
      "Epoch 15/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.4303 - acc: 0.5717Epoch 00014: val_loss did not improve\n",
      "256/256 [==============================] - 20s - loss: 1.4298 - acc: 0.5717 - val_loss: 1.2689 - val_acc: 0.6387\n",
      "Epoch 16/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.3906 - acc: 0.5902Epoch 00015: val_loss improved from 1.25012 to 1.16519, saving model to /output/training_logs/run-2/checkpoint-15-1.1652.hdf5\n",
      "256/256 [==============================] - 20s - loss: 1.3899 - acc: 0.5901 - val_loss: 1.1652 - val_acc: 0.6641\n",
      "Epoch 17/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.3818 - acc: 0.5966Epoch 00016: val_loss did not improve\n",
      "256/256 [==============================] - 19s - loss: 1.3822 - acc: 0.5967 - val_loss: 1.2838 - val_acc: 0.6387\n",
      "Epoch 18/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.3452 - acc: 0.6033Epoch 00017: val_loss improved from 1.16519 to 1.14806, saving model to /output/training_logs/run-2/checkpoint-17-1.1481.hdf5\n",
      "256/256 [==============================] - 19s - loss: 1.3450 - acc: 0.6031 - val_loss: 1.1481 - val_acc: 0.6738\n",
      "Epoch 19/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.3053 - acc: 0.6152Epoch 00018: val_loss improved from 1.14806 to 1.11013, saving model to /output/training_logs/run-2/checkpoint-18-1.1101.hdf5\n",
      "256/256 [==============================] - 19s - loss: 1.3059 - acc: 0.6155 - val_loss: 1.1101 - val_acc: 0.6875\n",
      "Epoch 20/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.2900 - acc: 0.6194Epoch 00019: val_loss improved from 1.11013 to 1.10679, saving model to /output/training_logs/run-2/checkpoint-19-1.1068.hdf5\n",
      "256/256 [==============================] - 19s - loss: 1.2911 - acc: 0.6198 - val_loss: 1.1068 - val_acc: 0.6699\n",
      "Epoch 21/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.2679 - acc: 0.6283Epoch 00020: val_loss did not improve\n",
      "256/256 [==============================] - 20s - loss: 1.2661 - acc: 0.6288 - val_loss: 1.1148 - val_acc: 0.6719\n",
      "Epoch 22/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.2467 - acc: 0.6357Epoch 00021: val_loss improved from 1.10679 to 1.08715, saving model to /output/training_logs/run-2/checkpoint-21-1.0871.hdf5\n",
      "256/256 [==============================] - 19s - loss: 1.2464 - acc: 0.6359 - val_loss: 1.0871 - val_acc: 0.6777\n",
      "Epoch 23/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.2360 - acc: 0.6362Epoch 00022: val_loss improved from 1.08715 to 1.03912, saving model to /output/training_logs/run-2/checkpoint-22-1.0391.hdf5\n",
      "256/256 [==============================] - 19s - loss: 1.2364 - acc: 0.6360 - val_loss: 1.0391 - val_acc: 0.6953\n",
      "Epoch 24/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.2294 - acc: 0.6374Epoch 00023: val_loss improved from 1.03912 to 0.96944, saving model to /output/training_logs/run-2/checkpoint-23-0.9694.hdf5\n",
      "256/256 [==============================] - 19s - loss: 1.2286 - acc: 0.6373 - val_loss: 0.9694 - val_acc: 0.7051\n",
      "Epoch 25/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.1931 - acc: 0.6529Epoch 00024: val_loss did not improve\n",
      "256/256 [==============================] - 20s - loss: 1.1912 - acc: 0.6537 - val_loss: 0.9865 - val_acc: 0.7109\n",
      "Epoch 26/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.1639 - acc: 0.6610Epoch 00025: val_loss did not improve\n",
      "256/256 [==============================] - 19s - loss: 1.1664 - acc: 0.6604 - val_loss: 0.9902 - val_acc: 0.7090\n",
      "Epoch 27/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.1734 - acc: 0.6555Epoch 00026: val_loss did not improve\n",
      "256/256 [==============================] - 19s - loss: 1.1728 - acc: 0.6559 - val_loss: 1.1185 - val_acc: 0.6855\n",
      "Epoch 00026: early stopping\n"
     ]
    }
   ],
   "source": [
    "# LSTM with heavy data augumentation(0.8 prob, / 3.0)\n",
    "m = get_cnn_rnn_model()\n",
    "m.summary()\n",
    "do_train(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 16000, 1)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 320, 64)           3264      \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 6, 64)             204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 6, 64)             256       \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 30)                990       \n",
      "=================================================================\n",
      "Total params: 221,790\n",
      "Trainable params: 221,662\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "RUN 6\n",
      "Epoch 1/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.1115 - acc: 0.1096Epoch 00000: val_loss improved from inf to 2.99122, saving model to /output/training_logs/run-6/checkpoint-00-2.9912.hdf5\n",
      "256/256 [==============================] - 17s - loss: 3.1113 - acc: 0.1093 - val_loss: 2.9912 - val_acc: 0.1602\n",
      "Epoch 2/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.6855 - acc: 0.2009- ETA: 1s - lEpoch 00001: val_loss improved from 2.99122 to 2.47296, saving model to /output/training_logs/run-6/checkpoint-01-2.4730.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.6851 - acc: 0.2010 - val_loss: 2.4730 - val_acc: 0.2676\n",
      "Epoch 3/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.3640 - acc: 0.2919Epoch 00002: val_loss improved from 2.47296 to 2.09064, saving model to /output/training_logs/run-6/checkpoint-02-2.0906.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.3635 - acc: 0.2920 - val_loss: 2.0906 - val_acc: 0.3848\n",
      "Epoch 4/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.1360 - acc: 0.3576Epoch 00003: val_loss improved from 2.09064 to 1.97470, saving model to /output/training_logs/run-6/checkpoint-03-1.9747.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.1361 - acc: 0.3579 - val_loss: 1.9747 - val_acc: 0.4316\n",
      "Epoch 5/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.9907 - acc: 0.4032Epoch 00004: val_loss improved from 1.97470 to 1.85570, saving model to /output/training_logs/run-6/checkpoint-04-1.8557.hdf5\n",
      "256/256 [==============================] - 16s - loss: 1.9908 - acc: 0.4031 - val_loss: 1.8557 - val_acc: 0.4531\n",
      "Epoch 6/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.8706 - acc: 0.4451Epoch 00005: val_loss improved from 1.85570 to 1.67761, saving model to /output/training_logs/run-6/checkpoint-05-1.6776.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.8723 - acc: 0.4445 - val_loss: 1.6776 - val_acc: 0.5176\n",
      "Epoch 7/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.7788 - acc: 0.4679Epoch 00006: val_loss improved from 1.67761 to 1.55472, saving model to /output/training_logs/run-6/checkpoint-06-1.5547.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.7776 - acc: 0.4679 - val_loss: 1.5547 - val_acc: 0.5645\n",
      "Epoch 8/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.6604 - acc: 0.5074Epoch 00007: val_loss improved from 1.55472 to 1.47000, saving model to /output/training_logs/run-6/checkpoint-07-1.4700.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.6591 - acc: 0.5079 - val_loss: 1.4700 - val_acc: 0.5801\n",
      "Epoch 9/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.6070 - acc: 0.5221Epoch 00008: val_loss improved from 1.47000 to 1.44583, saving model to /output/training_logs/run-6/checkpoint-08-1.4458.hdf5\n",
      "256/256 [==============================] - 16s - loss: 1.6056 - acc: 0.5223 - val_loss: 1.4458 - val_acc: 0.5801\n",
      "Epoch 10/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.5334 - acc: 0.5559Epoch 00009: val_loss improved from 1.44583 to 1.23588, saving model to /output/training_logs/run-6/checkpoint-09-1.2359.hdf5\n",
      "256/256 [==============================] - 16s - loss: 1.5330 - acc: 0.5562 - val_loss: 1.2359 - val_acc: 0.6445\n",
      "Epoch 11/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.4542 - acc: 0.5831Epoch 00010: val_loss improved from 1.23588 to 1.22829, saving model to /output/training_logs/run-6/checkpoint-10-1.2283.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.4543 - acc: 0.5833 - val_loss: 1.2283 - val_acc: 0.6562\n",
      "Epoch 12/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.3967 - acc: 0.5895Epoch 00011: val_loss improved from 1.22829 to 1.04270, saving model to /output/training_logs/run-6/checkpoint-11-1.0427.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.3965 - acc: 0.5895 - val_loss: 1.0427 - val_acc: 0.7168\n",
      "Epoch 13/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.3377 - acc: 0.6112Epoch 00012: val_loss did not improve\n",
      "256/256 [==============================] - 17s - loss: 1.3365 - acc: 0.6116 - val_loss: 1.0949 - val_acc: 0.6816\n",
      "Epoch 14/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.3069 - acc: 0.6130Epoch 00013: val_loss did not improve\n",
      "256/256 [==============================] - 16s - loss: 1.3061 - acc: 0.6133 - val_loss: 1.1661 - val_acc: 0.6406\n",
      "Epoch 15/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.2400 - acc: 0.6366Epoch 00014: val_loss did not improve\n",
      "256/256 [==============================] - 16s - loss: 1.2409 - acc: 0.6364 - val_loss: 1.1552 - val_acc: 0.6719\n",
      "Epoch 00014: early stopping\n"
     ]
    }
   ],
   "source": [
    "# LSTM with tiny data augumentation(0.5 prob, / 5.0)\n",
    "m = get_cnn_rnn_model()\n",
    "m.summary()\n",
    "do_train(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 16000, 1)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 320, 64)           3264      \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 6, 64)             204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 6, 64)             256       \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 30)                990       \n",
      "=================================================================\n",
      "Total params: 221,790\n",
      "Trainable params: 221,662\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "RUN 3\n",
      "Epoch 1/10000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 3.0938 - acc: 0.1240Epoch 00000: val_loss improved from inf to 2.95515, saving model to /output/training_logs/run-3/checkpoint-00-2.9551.hdf5\n",
      "256/256 [==============================] - 13s - loss: 3.0919 - acc: 0.1245 - val_loss: 2.9551 - val_acc: 0.1855\n",
      "Epoch 2/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.5948 - acc: 0.2310Epoch 00001: val_loss improved from 2.95515 to 2.40013, saving model to /output/training_logs/run-3/checkpoint-01-2.4001.hdf5\n",
      "256/256 [==============================] - 12s - loss: 2.5945 - acc: 0.2316 - val_loss: 2.4001 - val_acc: 0.2891\n",
      "Epoch 3/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.2370 - acc: 0.3249Epoch 00002: val_loss improved from 2.40013 to 2.12218, saving model to /output/training_logs/run-3/checkpoint-02-2.1222.hdf5\n",
      "256/256 [==============================] - 12s - loss: 2.2363 - acc: 0.3254 - val_loss: 2.1222 - val_acc: 0.3848\n",
      "Epoch 4/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.0234 - acc: 0.3886Epoch 00003: val_loss improved from 2.12218 to 1.96274, saving model to /output/training_logs/run-3/checkpoint-03-1.9627.hdf5\n",
      "256/256 [==============================] - 12s - loss: 2.0226 - acc: 0.3887 - val_loss: 1.9627 - val_acc: 0.4355\n",
      "Epoch 5/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.8418 - acc: 0.4507Epoch 00004: val_loss improved from 1.96274 to 1.74447, saving model to /output/training_logs/run-3/checkpoint-04-1.7445.hdf5\n",
      "256/256 [==============================] - 12s - loss: 1.8398 - acc: 0.4515 - val_loss: 1.7445 - val_acc: 0.4531\n",
      "Epoch 6/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.7241 - acc: 0.4915Epoch 00005: val_loss did not improve\n",
      "256/256 [==============================] - 12s - loss: 1.7241 - acc: 0.4908 - val_loss: 1.9461 - val_acc: 0.4102\n",
      "Epoch 7/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.5820 - acc: 0.5430Epoch 00006: val_loss improved from 1.74447 to 1.32576, saving model to /output/training_logs/run-3/checkpoint-06-1.3258.hdf5\n",
      "256/256 [==============================] - 12s - loss: 1.5823 - acc: 0.5431 - val_loss: 1.3258 - val_acc: 0.6641\n",
      "Epoch 8/10000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 1.4935 - acc: 0.5725Epoch 00007: val_loss did not improve\n",
      "256/256 [==============================] - 12s - loss: 1.4941 - acc: 0.5724 - val_loss: 1.3940 - val_acc: 0.6348\n",
      "Epoch 9/10000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 1.4203 - acc: 0.5919Epoch 00008: val_loss improved from 1.32576 to 1.18293, saving model to /output/training_logs/run-3/checkpoint-08-1.1829.hdf5\n",
      "256/256 [==============================] - 12s - loss: 1.4179 - acc: 0.5927 - val_loss: 1.1829 - val_acc: 0.6855\n",
      "Epoch 10/10000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 1.3241 - acc: 0.6228Epoch 00009: val_loss improved from 1.18293 to 1.14734, saving model to /output/training_logs/run-3/checkpoint-09-1.1473.hdf5\n",
      "256/256 [==============================] - 12s - loss: 1.3228 - acc: 0.6228 - val_loss: 1.1473 - val_acc: 0.6777\n",
      "Epoch 11/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.2850 - acc: 0.6306Epoch 00010: val_loss did not improve\n",
      "256/256 [==============================] - 12s - loss: 1.2840 - acc: 0.6307 - val_loss: 1.2014 - val_acc: 0.6875\n",
      "Epoch 12/10000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 1.2385 - acc: 0.6506Epoch 00011: val_loss improved from 1.14734 to 1.07263, saving model to /output/training_logs/run-3/checkpoint-11-1.0726.hdf5\n",
      "256/256 [==============================] - 12s - loss: 1.2390 - acc: 0.6504 - val_loss: 1.0726 - val_acc: 0.6699\n",
      "Epoch 13/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.1645 - acc: 0.6632Epoch 00012: val_loss did not improve\n",
      "256/256 [==============================] - 12s - loss: 1.1657 - acc: 0.6631 - val_loss: 1.1870 - val_acc: 0.6758\n",
      "Epoch 14/10000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 1.1349 - acc: 0.6785Epoch 00013: val_loss did not improve\n",
      "256/256 [==============================] - 12s - loss: 1.1350 - acc: 0.6782 - val_loss: 1.1251 - val_acc: 0.7129\n",
      "Epoch 15/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.0860 - acc: 0.6902Epoch 00014: val_loss improved from 1.07263 to 1.04053, saving model to /output/training_logs/run-3/checkpoint-14-1.0405.hdf5\n",
      "256/256 [==============================] - 12s - loss: 1.0881 - acc: 0.6897 - val_loss: 1.0405 - val_acc: 0.7012\n",
      "Epoch 16/10000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 1.0676 - acc: 0.6930Epoch 00015: val_loss improved from 1.04053 to 0.93197, saving model to /output/training_logs/run-3/checkpoint-15-0.9320.hdf5\n",
      "256/256 [==============================] - 12s - loss: 1.0690 - acc: 0.6927 - val_loss: 0.9320 - val_acc: 0.7402\n",
      "Epoch 17/10000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 1.0557 - acc: 0.7003Epoch 00016: val_loss did not improve\n",
      "256/256 [==============================] - 12s - loss: 1.0540 - acc: 0.7002 - val_loss: 0.9354 - val_acc: 0.7363\n",
      "Epoch 18/10000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 1.0397 - acc: 0.7034Epoch 00017: val_loss improved from 0.93197 to 0.81352, saving model to /output/training_logs/run-3/checkpoint-17-0.8135.hdf5\n",
      "256/256 [==============================] - 12s - loss: 1.0406 - acc: 0.7030 - val_loss: 0.8135 - val_acc: 0.7539\n",
      "Epoch 19/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.0276 - acc: 0.7091Epoch 00018: val_loss did not improve\n",
      "256/256 [==============================] - 12s - loss: 1.0277 - acc: 0.7090 - val_loss: 1.0056 - val_acc: 0.6855\n",
      "Epoch 20/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 0.9585 - acc: 0.7268Epoch 00019: val_loss did not improve\n",
      "256/256 [==============================] - 12s - loss: 0.9583 - acc: 0.7273 - val_loss: 0.8872 - val_acc: 0.7598\n",
      "Epoch 21/10000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 0.9636 - acc: 0.7238Epoch 00020: val_loss did not improve\n",
      "256/256 [==============================] - 12s - loss: 0.9634 - acc: 0.7242 - val_loss: 0.8994 - val_acc: 0.7500\n",
      "Epoch 00020: early stopping\n"
     ]
    }
   ],
   "source": [
    "# LSTM without data augumentation[Best Model]\n",
    "m = get_cnn_rnn_model()\n",
    "m.summary()\n",
    "do_train(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 16000, 1)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 320, 64)           3264      \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 6, 64)             204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 6, 64)             256       \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 30)                990       \n",
      "=================================================================\n",
      "Total params: 221,790\n",
      "Trainable params: 221,662\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "RUN 7\n",
      "Epoch 1/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.2839 - acc: 0.0777- ETA: 0s - loss: 3.2852 - acc: 0.07Epoch 00000: val_loss improved from inf to 3.25793, saving model to /output/training_logs/run-7/checkpoint-00-3.2579.hdf5\n",
      "256/256 [==============================] - 18s - loss: 3.2831 - acc: 0.0782 - val_loss: 3.2579 - val_acc: 0.1094\n",
      "Epoch 2/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.0121 - acc: 0.1306Epoch 00001: val_loss improved from 3.25793 to 2.91162, saving model to /output/training_logs/run-7/checkpoint-01-2.9116.hdf5\n",
      "256/256 [==============================] - 17s - loss: 3.0127 - acc: 0.1305 - val_loss: 2.9116 - val_acc: 0.1660\n",
      "Epoch 3/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.8259 - acc: 0.1701Epoch 00002: val_loss improved from 2.91162 to 2.72312, saving model to /output/training_logs/run-7/checkpoint-02-2.7231.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.8252 - acc: 0.1702 - val_loss: 2.7231 - val_acc: 0.2129\n",
      "Epoch 4/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.6881 - acc: 0.2048Epoch 00003: val_loss improved from 2.72312 to 2.47573, saving model to /output/training_logs/run-7/checkpoint-03-2.4757.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.6880 - acc: 0.2048 - val_loss: 2.4757 - val_acc: 0.2637\n",
      "Epoch 5/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.5546 - acc: 0.2354Epoch 00004: val_loss improved from 2.47573 to 2.43382, saving model to /output/training_logs/run-7/checkpoint-04-2.4338.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.5548 - acc: 0.2352 - val_loss: 2.4338 - val_acc: 0.2754\n",
      "Epoch 6/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.4886 - acc: 0.2406Epoch 00005: val_loss improved from 2.43382 to 2.28302, saving model to /output/training_logs/run-7/checkpoint-05-2.2830.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.4894 - acc: 0.2402 - val_loss: 2.2830 - val_acc: 0.3555\n",
      "Epoch 7/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.4099 - acc: 0.2708- ETA: 1s - losEpoch 00006: val_loss improved from 2.28302 to 2.27167, saving model to /output/training_logs/run-7/checkpoint-06-2.2717.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.4102 - acc: 0.2706 - val_loss: 2.2717 - val_acc: 0.3125\n",
      "Epoch 8/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.3407 - acc: 0.2934Epoch 00007: val_loss improved from 2.27167 to 2.06387, saving model to /output/training_logs/run-7/checkpoint-07-2.0639.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.3397 - acc: 0.2939 - val_loss: 2.0639 - val_acc: 0.3750\n",
      "Epoch 9/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.3014 - acc: 0.2952Epoch 00008: val_loss did not improve\n",
      "256/256 [==============================] - 17s - loss: 2.3016 - acc: 0.2953 - val_loss: 2.1130 - val_acc: 0.3926\n",
      "Epoch 10/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.2115 - acc: 0.3268Epoch 00009: val_loss improved from 2.06387 to 2.00945, saving model to /output/training_logs/run-7/checkpoint-09-2.0095.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.2111 - acc: 0.3274 - val_loss: 2.0095 - val_acc: 0.4121\n",
      "Epoch 11/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.1895 - acc: 0.3349Epoch 00010: val_loss did not improve\n",
      "256/256 [==============================] - 17s - loss: 2.1887 - acc: 0.3350 - val_loss: 2.0327 - val_acc: 0.4453\n",
      "Epoch 12/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.1422 - acc: 0.3540Epoch 00011: val_loss improved from 2.00945 to 1.89995, saving model to /output/training_logs/run-7/checkpoint-11-1.8999.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.1422 - acc: 0.3536 - val_loss: 1.8999 - val_acc: 0.4180\n",
      "Epoch 13/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.0853 - acc: 0.3635Epoch 00012: val_loss improved from 1.89995 to 1.83937, saving model to /output/training_logs/run-7/checkpoint-12-1.8394.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.0846 - acc: 0.3639 - val_loss: 1.8394 - val_acc: 0.4531\n",
      "Epoch 14/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.0501 - acc: 0.3789Epoch 00013: val_loss improved from 1.83937 to 1.77355, saving model to /output/training_logs/run-7/checkpoint-13-1.7735.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.0488 - acc: 0.3794 - val_loss: 1.7735 - val_acc: 0.4668\n",
      "Epoch 15/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.0160 - acc: 0.3924Epoch 00014: val_loss improved from 1.77355 to 1.77021, saving model to /output/training_logs/run-7/checkpoint-14-1.7702.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.0156 - acc: 0.3928 - val_loss: 1.7702 - val_acc: 0.4902\n",
      "Epoch 16/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.0099 - acc: 0.4004Epoch 00015: val_loss improved from 1.77021 to 1.72491, saving model to /output/training_logs/run-7/checkpoint-15-1.7249.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.0108 - acc: 0.4001 - val_loss: 1.7249 - val_acc: 0.5020\n",
      "Epoch 17/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.9554 - acc: 0.4088Epoch 00016: val_loss improved from 1.72491 to 1.68741, saving model to /output/training_logs/run-7/checkpoint-16-1.6874.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.9556 - acc: 0.4091 - val_loss: 1.6874 - val_acc: 0.5020\n",
      "Epoch 18/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.9111 - acc: 0.4180Epoch 00017: val_loss improved from 1.68741 to 1.65394, saving model to /output/training_logs/run-7/checkpoint-17-1.6539.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.9115 - acc: 0.4180 - val_loss: 1.6539 - val_acc: 0.5410\n",
      "Epoch 19/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.8955 - acc: 0.4255Epoch 00018: val_loss did not improve\n",
      "256/256 [==============================] - 17s - loss: 1.8951 - acc: 0.4259 - val_loss: 1.7206 - val_acc: 0.4766\n",
      "Epoch 20/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.8749 - acc: 0.4362Epoch 00019: val_loss did not improve\n",
      "256/256 [==============================] - 17s - loss: 1.8759 - acc: 0.4362 - val_loss: 1.6618 - val_acc: 0.5312\n",
      "Epoch 21/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.8280 - acc: 0.4518Epoch 00020: val_loss improved from 1.65394 to 1.58409, saving model to /output/training_logs/run-7/checkpoint-20-1.5841.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.8285 - acc: 0.4518 - val_loss: 1.5841 - val_acc: 0.5430\n",
      "Epoch 22/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.8171 - acc: 0.4504Epoch 00021: val_loss improved from 1.58409 to 1.56737, saving model to /output/training_logs/run-7/checkpoint-21-1.5674.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.8154 - acc: 0.4509 - val_loss: 1.5674 - val_acc: 0.5898\n",
      "Epoch 23/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.8138 - acc: 0.4556Epoch 00022: val_loss improved from 1.56737 to 1.55019, saving model to /output/training_logs/run-7/checkpoint-22-1.5502.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.8138 - acc: 0.4554 - val_loss: 1.5502 - val_acc: 0.5625\n",
      "Epoch 24/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.8029 - acc: 0.4607Epoch 00023: val_loss did not improve\n",
      "256/256 [==============================] - 17s - loss: 1.8026 - acc: 0.4604 - val_loss: 1.5732 - val_acc: 0.5547\n",
      "Epoch 25/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.7736 - acc: 0.4580Epoch 00024: val_loss improved from 1.55019 to 1.54312, saving model to /output/training_logs/run-7/checkpoint-24-1.5431.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.7741 - acc: 0.4576 - val_loss: 1.5431 - val_acc: 0.5723\n",
      "Epoch 26/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.7422 - acc: 0.4757Epoch 00025: val_loss improved from 1.54312 to 1.47101, saving model to /output/training_logs/run-7/checkpoint-25-1.4710.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.7421 - acc: 0.4757 - val_loss: 1.4710 - val_acc: 0.5801\n",
      "Epoch 27/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.7173 - acc: 0.4887Epoch 00026: val_loss improved from 1.47101 to 1.43705, saving model to /output/training_logs/run-7/checkpoint-26-1.4370.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.7170 - acc: 0.4885 - val_loss: 1.4370 - val_acc: 0.6113\n",
      "Epoch 28/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.6924 - acc: 0.4964Epoch 00027: val_loss improved from 1.43705 to 1.37850, saving model to /output/training_logs/run-7/checkpoint-27-1.3785.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.6919 - acc: 0.4967 - val_loss: 1.3785 - val_acc: 0.5879\n",
      "Epoch 29/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.6536 - acc: 0.5074Epoch 00028: val_loss did not improve\n",
      "256/256 [==============================] - 17s - loss: 1.6545 - acc: 0.5067 - val_loss: 1.4066 - val_acc: 0.5898\n",
      "Epoch 30/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.6556 - acc: 0.5013Epoch 00029: val_loss improved from 1.37850 to 1.37463, saving model to /output/training_logs/run-7/checkpoint-29-1.3746.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.6561 - acc: 0.5011 - val_loss: 1.3746 - val_acc: 0.6230\n",
      "Epoch 31/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.6405 - acc: 0.5143Epoch 00030: val_loss improved from 1.37463 to 1.33813, saving model to /output/training_logs/run-7/checkpoint-30-1.3381.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.6410 - acc: 0.5139 - val_loss: 1.3381 - val_acc: 0.6211\n",
      "Epoch 32/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.6358 - acc: 0.5123Epoch 00031: val_loss did not improve\n",
      "256/256 [==============================] - 17s - loss: 1.6370 - acc: 0.5122 - val_loss: 1.3728 - val_acc: 0.6211\n",
      "Epoch 33/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.6267 - acc: 0.5167Epoch 00032: val_loss improved from 1.33813 to 1.22830, saving model to /output/training_logs/run-7/checkpoint-32-1.2283.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.6276 - acc: 0.5165 - val_loss: 1.2283 - val_acc: 0.6426\n",
      "Epoch 34/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.5847 - acc: 0.5272Epoch 00033: val_loss did not improve\n",
      "256/256 [==============================] - 17s - loss: 1.5846 - acc: 0.5272 - val_loss: 1.2598 - val_acc: 0.6465\n",
      "Epoch 35/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.5850 - acc: 0.5328Epoch 00034: val_loss did not improve\n",
      "256/256 [==============================] - 17s - loss: 1.5852 - acc: 0.5326 - val_loss: 1.4041 - val_acc: 0.5840\n",
      "Epoch 36/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.5492 - acc: 0.5455- ETA: 0s - loss: 1.5496 -Epoch 00035: val_loss improved from 1.22830 to 1.19272, saving model to /output/training_logs/run-7/checkpoint-35-1.1927.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.5488 - acc: 0.5454 - val_loss: 1.1927 - val_acc: 0.6582\n",
      "Epoch 37/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.5469 - acc: 0.5449Epoch 00036: val_loss did not improve\n",
      "256/256 [==============================] - 17s - loss: 1.5479 - acc: 0.5448 - val_loss: 1.2818 - val_acc: 0.6309\n",
      "Epoch 38/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.4959 - acc: 0.5600Epoch 00037: val_loss did not improve\n",
      "256/256 [==============================] - 17s - loss: 1.4953 - acc: 0.5599 - val_loss: 1.2124 - val_acc: 0.6602\n",
      "Epoch 39/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.5153 - acc: 0.5532Epoch 00038: val_loss did not improve\n",
      "256/256 [==============================] - 17s - loss: 1.5156 - acc: 0.5527 - val_loss: 1.2467 - val_acc: 0.6484\n",
      "Epoch 00038: early stopping\n"
     ]
    }
   ],
   "source": [
    "# LSTM with dropout\n",
    "m = get_cnn_rnn_model()\n",
    "m.summary()\n",
    "do_train(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 16000, 1)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 320, 64)           3264      \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 6, 64)             204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 6, 64)             256       \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 32)                9312      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                990       \n",
      "=================================================================\n",
      "Total params: 218,686\n",
      "Trainable params: 218,558\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "RUN 2\n",
      "Epoch 1/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.2504 - acc: 0.0783Epoch 00000: val_loss improved from inf to 3.22532, saving model to /output/training_logs/run-2/checkpoint-00-3.2253.hdf5\n",
      "256/256 [==============================] - 17s - loss: 3.2497 - acc: 0.0786 - val_loss: 3.2253 - val_acc: 0.0918\n",
      "Epoch 2/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.9148 - acc: 0.1415Epoch 00001: val_loss improved from 3.22532 to 2.78750, saving model to /output/training_logs/run-2/checkpoint-01-2.7875.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.9136 - acc: 0.1422 - val_loss: 2.7875 - val_acc: 0.1582\n",
      "Epoch 3/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.6805 - acc: 0.2036Epoch 00002: val_loss improved from 2.78750 to 2.50273, saving model to /output/training_logs/run-2/checkpoint-02-2.5027.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.6794 - acc: 0.2035 - val_loss: 2.5027 - val_acc: 0.2500\n",
      "Epoch 4/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.4960 - acc: 0.2493- ETAEpoch 00003: val_loss improved from 2.50273 to 2.39827, saving model to /output/training_logs/run-2/checkpoint-03-2.3983.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.4957 - acc: 0.2494 - val_loss: 2.3983 - val_acc: 0.2930\n",
      "Epoch 5/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.3925 - acc: 0.2746- ETA: 2s - loss: 2.4046 - - ETA: 1s - lEpoch 00004: val_loss improved from 2.39827 to 2.23867, saving model to /output/training_logs/run-2/checkpoint-04-2.2387.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.3932 - acc: 0.2744 - val_loss: 2.2387 - val_acc: 0.3262\n",
      "Epoch 6/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.2821 - acc: 0.3110Epoch 00005: val_loss improved from 2.23867 to 2.13931, saving model to /output/training_logs/run-2/checkpoint-05-2.1393.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.2813 - acc: 0.3119 - val_loss: 2.1393 - val_acc: 0.4062\n",
      "Epoch 7/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.1773 - acc: 0.3441Epoch 00006: val_loss improved from 2.13931 to 2.08981, saving model to /output/training_logs/run-2/checkpoint-06-2.0898.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.1775 - acc: 0.3441 - val_loss: 2.0898 - val_acc: 0.3984\n",
      "Epoch 8/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.0939 - acc: 0.3741- ETA: 5s - loss: 2.0998 Epoch 00007: val_loss improved from 2.08981 to 1.98534, saving model to /output/training_logs/run-2/checkpoint-07-1.9853.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.0939 - acc: 0.3739 - val_loss: 1.9853 - val_acc: 0.4316\n",
      "Epoch 9/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.0255 - acc: 0.394 - ETA: 0s - loss: 2.0250 - acc: 0.3951Epoch 00008: val_loss improved from 1.98534 to 1.92817, saving model to /output/training_logs/run-2/checkpoint-08-1.9282.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.0245 - acc: 0.3950 - val_loss: 1.9282 - val_acc: 0.4727\n",
      "Epoch 10/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.9710 - acc: 0.4082Epoch 00009: val_loss improved from 1.92817 to 1.79519, saving model to /output/training_logs/run-2/checkpoint-09-1.7952.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.9696 - acc: 0.4086 - val_loss: 1.7952 - val_acc: 0.5059\n",
      "Epoch 11/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.9007 - acc: 0.4391Epoch 00010: val_loss improved from 1.79519 to 1.74692, saving model to /output/training_logs/run-2/checkpoint-10-1.7469.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.9007 - acc: 0.4391 - val_loss: 1.7469 - val_acc: 0.5312\n",
      "Epoch 12/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.8238 - acc: 0.4556Epoch 00011: val_loss improved from 1.74692 to 1.72313, saving model to /output/training_logs/run-2/checkpoint-11-1.7231.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.8233 - acc: 0.4558 - val_loss: 1.7231 - val_acc: 0.5254\n",
      "Epoch 13/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.7924 - acc: 0.4661Epoch 00012: val_loss improved from 1.72313 to 1.59142, saving model to /output/training_logs/run-2/checkpoint-12-1.5914.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.7928 - acc: 0.4661 - val_loss: 1.5914 - val_acc: 0.5645\n",
      "Epoch 14/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.7309 - acc: 0.4825Epoch 00013: val_loss improved from 1.59142 to 1.56105, saving model to /output/training_logs/run-2/checkpoint-13-1.5610.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.7302 - acc: 0.4824 - val_loss: 1.5610 - val_acc: 0.5566\n",
      "Epoch 15/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.6883 - acc: 0.4949Epoch 00014: val_loss improved from 1.56105 to 1.56050, saving model to /output/training_logs/run-2/checkpoint-14-1.5605.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.6878 - acc: 0.4946 - val_loss: 1.5605 - val_acc: 0.5781\n",
      "Epoch 16/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.6360 - acc: 0.5147Epoch 00015: val_loss improved from 1.56050 to 1.47583, saving model to /output/training_logs/run-2/checkpoint-15-1.4758.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.6355 - acc: 0.5144 - val_loss: 1.4758 - val_acc: 0.5684\n",
      "Epoch 17/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.6316 - acc: 0.5146Epoch 00016: val_loss did not improve\n",
      "256/256 [==============================] - 17s - loss: 1.6338 - acc: 0.5137 - val_loss: 1.5257 - val_acc: 0.5820\n",
      "Epoch 18/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.5668 - acc: 0.5382Epoch 00017: val_loss improved from 1.47583 to 1.45653, saving model to /output/training_logs/run-2/checkpoint-17-1.4565.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.5668 - acc: 0.5380 - val_loss: 1.4565 - val_acc: 0.5879\n",
      "Epoch 19/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.5469 - acc: 0.543 - ETA: 0s - loss: 1.5467 - acc: 0.5440Epoch 00018: val_loss improved from 1.45653 to 1.35995, saving model to /output/training_logs/run-2/checkpoint-18-1.3599.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.5479 - acc: 0.5438 - val_loss: 1.3599 - val_acc: 0.6074\n",
      "Epoch 20/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.5212 - acc: 0.5485Epoch 00019: val_loss did not improve\n",
      "256/256 [==============================] - 17s - loss: 1.5207 - acc: 0.5487 - val_loss: 1.4018 - val_acc: 0.6074\n",
      "Epoch 21/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.4790 - acc: 0.5631Epoch 00020: val_loss improved from 1.35995 to 1.29239, saving model to /output/training_logs/run-2/checkpoint-20-1.2924.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.4793 - acc: 0.5630 - val_loss: 1.2924 - val_acc: 0.6230\n",
      "Epoch 22/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.4849 - acc: 0.5532Epoch 00021: val_loss did not improve\n",
      "256/256 [==============================] - 17s - loss: 1.4848 - acc: 0.5533 - val_loss: 1.3393 - val_acc: 0.5996\n",
      "Epoch 23/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.4397 - acc: 0.5765Epoch 00022: val_loss improved from 1.29239 to 1.20273, saving model to /output/training_logs/run-2/checkpoint-22-1.2027.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.4386 - acc: 0.5767 - val_loss: 1.2027 - val_acc: 0.6680\n",
      "Epoch 24/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.4209 - acc: 0.5880Epoch 00023: val_loss did not improve\n",
      "256/256 [==============================] - 17s - loss: 1.4206 - acc: 0.5880 - val_loss: 1.3021 - val_acc: 0.6309\n",
      "Epoch 25/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.3864 - acc: 0.5998Epoch 00024: val_loss did not improve\n",
      "256/256 [==============================] - 17s - loss: 1.3871 - acc: 0.5995 - val_loss: 1.2249 - val_acc: 0.6523\n",
      "Epoch 26/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.3438 - acc: 0.6037Epoch 00025: val_loss did not improve\n",
      "256/256 [==============================] - 17s - loss: 1.3433 - acc: 0.6036 - val_loss: 1.2046 - val_acc: 0.6641\n",
      "Epoch 00025: early stopping\n"
     ]
    }
   ],
   "source": [
    "# GRU\n",
    "m = get_cnn_rnn_model()\n",
    "m.summary()\n",
    "do_train(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 16000, 1)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 320, 64)           3264      \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 6, 64)             204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 6, 64)             256       \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 32)                9312      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 30)                990       \n",
      "=================================================================\n",
      "Total params: 218,686\n",
      "Trainable params: 218,558\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "RUN 3\n",
      "Epoch 1/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.2445 - acc: 0.0792Epoch 00000: val_loss improved from inf to 3.24772, saving model to /output/training_logs/run-3/checkpoint-00-3.2477.hdf5\n",
      "256/256 [==============================] - 17s - loss: 3.2444 - acc: 0.0791 - val_loss: 3.2477 - val_acc: 0.1367\n",
      "Epoch 2/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.9152 - acc: 0.1444Epoch 00001: val_loss improved from 3.24772 to 2.75880, saving model to /output/training_logs/run-3/checkpoint-01-2.7588.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.9142 - acc: 0.1444 - val_loss: 2.7588 - val_acc: 0.2012\n",
      "Epoch 3/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.6687 - acc: 0.2051Epoch 00002: val_loss improved from 2.75880 to 2.53592, saving model to /output/training_logs/run-3/checkpoint-02-2.5359.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.6682 - acc: 0.2054 - val_loss: 2.5359 - val_acc: 0.2246\n",
      "Epoch 4/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.4702 - acc: 0.2719Epoch 00003: val_loss improved from 2.53592 to 2.36132, saving model to /output/training_logs/run-3/checkpoint-03-2.3613.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.4694 - acc: 0.2728 - val_loss: 2.3613 - val_acc: 0.2832\n",
      "Epoch 5/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.3109 - acc: 0.3118Epoch 00004: val_loss improved from 2.36132 to 2.17850, saving model to /output/training_logs/run-3/checkpoint-04-2.1785.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.3099 - acc: 0.3123 - val_loss: 2.1785 - val_acc: 0.3965\n",
      "Epoch 6/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.1931 - acc: 0.3627Epoch 00005: val_loss improved from 2.17850 to 2.10987, saving model to /output/training_logs/run-3/checkpoint-05-2.1099.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.1923 - acc: 0.3630 - val_loss: 2.1099 - val_acc: 0.3848\n",
      "Epoch 7/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.0983 - acc: 0.3830Epoch 00006: val_loss improved from 2.10987 to 1.95361, saving model to /output/training_logs/run-3/checkpoint-06-1.9536.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.0982 - acc: 0.3831 - val_loss: 1.9536 - val_acc: 0.4512\n",
      "Epoch 8/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.0124 - acc: 0.3989Epoch 00007: val_loss improved from 1.95361 to 1.83906, saving model to /output/training_logs/run-3/checkpoint-07-1.8391.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.0123 - acc: 0.3992 - val_loss: 1.8391 - val_acc: 0.4883\n",
      "Epoch 9/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.9217 - acc: 0.4306Epoch 00008: val_loss improved from 1.83906 to 1.80918, saving model to /output/training_logs/run-3/checkpoint-08-1.8092.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.9207 - acc: 0.4312 - val_loss: 1.8092 - val_acc: 0.4785\n",
      "Epoch 10/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.8684 - acc: 0.4494Epoch 00009: val_loss improved from 1.80918 to 1.73086, saving model to /output/training_logs/run-3/checkpoint-09-1.7309.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.8691 - acc: 0.4493 - val_loss: 1.7309 - val_acc: 0.5215\n",
      "Epoch 11/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.8118 - acc: 0.4697Epoch 00010: val_loss improved from 1.73086 to 1.72494, saving model to /output/training_logs/run-3/checkpoint-10-1.7249.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.8116 - acc: 0.4694 - val_loss: 1.7249 - val_acc: 0.5352\n",
      "Epoch 12/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.7382 - acc: 0.4885Epoch 00011: val_loss improved from 1.72494 to 1.62135, saving model to /output/training_logs/run-3/checkpoint-11-1.6214.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.7368 - acc: 0.4891 - val_loss: 1.6214 - val_acc: 0.5371\n",
      "Epoch 13/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.6893 - acc: 0.5071Epoch 00012: val_loss did not improve\n",
      "256/256 [==============================] - 17s - loss: 1.6892 - acc: 0.5077 - val_loss: 1.6343 - val_acc: 0.5391\n",
      "Epoch 14/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.6371 - acc: 0.5230Epoch 00013: val_loss improved from 1.62135 to 1.48476, saving model to /output/training_logs/run-3/checkpoint-13-1.4848.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.6357 - acc: 0.5234 - val_loss: 1.4848 - val_acc: 0.5723\n",
      "Epoch 15/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.5885 - acc: 0.5354Epoch 00014: val_loss improved from 1.48476 to 1.41363, saving model to /output/training_logs/run-3/checkpoint-14-1.4136.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.5886 - acc: 0.5355 - val_loss: 1.4136 - val_acc: 0.6113\n",
      "Epoch 16/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.5733 - acc: 0.5434Epoch 00015: val_loss improved from 1.41363 to 1.37376, saving model to /output/training_logs/run-3/checkpoint-15-1.3738.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.5733 - acc: 0.5433 - val_loss: 1.3738 - val_acc: 0.6309\n",
      "Epoch 17/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.5105 - acc: 0.5641Epoch 00016: val_loss did not improve\n",
      "256/256 [==============================] - 16s - loss: 1.5103 - acc: 0.5640 - val_loss: 1.4792 - val_acc: 0.6113\n",
      "Epoch 18/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.4582 - acc: 0.5749Epoch 00017: val_loss improved from 1.37376 to 1.34463, saving model to /output/training_logs/run-3/checkpoint-17-1.3446.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.4567 - acc: 0.5754 - val_loss: 1.3446 - val_acc: 0.6152\n",
      "Epoch 19/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.4421 - acc: 0.5869Epoch 00018: val_loss improved from 1.34463 to 1.30746, saving model to /output/training_logs/run-3/checkpoint-18-1.3075.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.4434 - acc: 0.5862 - val_loss: 1.3075 - val_acc: 0.6289\n",
      "Epoch 20/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.4381 - acc: 0.5821Epoch 00019: val_loss improved from 1.30746 to 1.28247, saving model to /output/training_logs/run-3/checkpoint-19-1.2825.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.4375 - acc: 0.5826 - val_loss: 1.2825 - val_acc: 0.6445\n",
      "Epoch 21/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.4129 - acc: 0.5893Epoch 00020: val_loss improved from 1.28247 to 1.26445, saving model to /output/training_logs/run-3/checkpoint-20-1.2644.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.4124 - acc: 0.5894 - val_loss: 1.2644 - val_acc: 0.6543\n",
      "Epoch 22/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.3728 - acc: 0.6037Epoch 00021: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 17s - loss: 1.3741 - acc: 0.6034 - val_loss: 1.3062 - val_acc: 0.6641\n",
      "Epoch 23/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.3239 - acc: 0.6097Epoch 00022: val_loss improved from 1.26445 to 1.24151, saving model to /output/training_logs/run-3/checkpoint-22-1.2415.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.3244 - acc: 0.6094 - val_loss: 1.2415 - val_acc: 0.6309\n",
      "Epoch 24/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.2923 - acc: 0.6259Epoch 00023: val_loss improved from 1.24151 to 1.17432, saving model to /output/training_logs/run-3/checkpoint-23-1.1743.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.2925 - acc: 0.6256 - val_loss: 1.1743 - val_acc: 0.6758\n",
      "Epoch 25/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.2844 - acc: 0.6322Epoch 00024: val_loss did not improve\n",
      "256/256 [==============================] - 17s - loss: 1.2840 - acc: 0.6328 - val_loss: 1.1768 - val_acc: 0.6797\n",
      "Epoch 26/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.2418 - acc: 0.6424Epoch 00025: val_loss improved from 1.17432 to 1.12419, saving model to /output/training_logs/run-3/checkpoint-25-1.1242.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.2410 - acc: 0.6425 - val_loss: 1.1242 - val_acc: 0.6934\n",
      "Epoch 27/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.2553 - acc: 0.6387Epoch 00026: val_loss improved from 1.12419 to 1.01877, saving model to /output/training_logs/run-3/checkpoint-26-1.0188.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.2560 - acc: 0.6390 - val_loss: 1.0188 - val_acc: 0.7246\n",
      "Epoch 28/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.2260 - acc: 0.6447Epoch 00027: val_loss did not improve\n",
      "256/256 [==============================] - 17s - loss: 1.2261 - acc: 0.6447 - val_loss: 1.0796 - val_acc: 0.7051\n",
      "Epoch 29/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.1964 - acc: 0.6574Epoch 00028: val_loss did not improve\n",
      "256/256 [==============================] - 17s - loss: 1.1958 - acc: 0.6576 - val_loss: 1.0995 - val_acc: 0.6719\n",
      "Epoch 30/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.1815 - acc: 0.6612Epoch 00029: val_loss did not improve\n",
      "256/256 [==============================] - 17s - loss: 1.1808 - acc: 0.6615 - val_loss: 1.0340 - val_acc: 0.7305\n",
      "Epoch 00029: early stopping\n"
     ]
    }
   ],
   "source": [
    "# double LSTM\n",
    "m = get_cnn_rnn_model()\n",
    "m.summary()\n",
    "do_train(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, 16000, 1)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)                (None, 320, 64)       3264        input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)                (None, 6, 64)         204864      conv1d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 6, 64)         256         conv1d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 6, 32)         12416       batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                    (None, 6, 32)         12416       batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 6, 64)         0           lstm_1[0][0]                     \n",
      "                                                                   lstm_2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 384)           0           concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 384)           0           flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 30)            11550       dropout_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 244,766\n",
      "Trainable params: 244,638\n",
      "Non-trainable params: 128\n",
      "____________________________________________________________________________________________________\n",
      "RUN 4\n",
      "Epoch 1/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 3.0808 - acc: 0.1196Epoch 00000: val_loss improved from inf to 3.02464, saving model to /output/training_logs/run-4/checkpoint-00-3.0246.hdf5\n",
      "256/256 [==============================] - 17s - loss: 3.0798 - acc: 0.1198 - val_loss: 3.0246 - val_acc: 0.1836\n",
      "Epoch 2/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.6367 - acc: 0.2082Epoch 00001: val_loss improved from 3.02464 to 2.50841, saving model to /output/training_logs/run-4/checkpoint-01-2.5084.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.6363 - acc: 0.2081 - val_loss: 2.5084 - val_acc: 0.2539\n",
      "Epoch 3/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.4025 - acc: 0.2804Epoch 00002: val_loss improved from 2.50841 to 2.21341, saving model to /output/training_logs/run-4/checkpoint-02-2.2134.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.4018 - acc: 0.2808 - val_loss: 2.2134 - val_acc: 0.3711\n",
      "Epoch 4/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.1791 - acc: 0.3438Epoch 00003: val_loss improved from 2.21341 to 2.03137, saving model to /output/training_logs/run-4/checkpoint-03-2.0314.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.1801 - acc: 0.3434 - val_loss: 2.0314 - val_acc: 0.4102\n",
      "Epoch 5/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 2.0237 - acc: 0.3952Epoch 00004: val_loss improved from 2.03137 to 1.84869, saving model to /output/training_logs/run-4/checkpoint-04-1.8487.hdf5\n",
      "256/256 [==============================] - 17s - loss: 2.0229 - acc: 0.3956 - val_loss: 1.8487 - val_acc: 0.4492\n",
      "Epoch 6/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.8765 - acc: 0.4386Epoch 00005: val_loss improved from 1.84869 to 1.83951, saving model to /output/training_logs/run-4/checkpoint-05-1.8395.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.8774 - acc: 0.4382 - val_loss: 1.8395 - val_acc: 0.4648\n",
      "Epoch 7/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.7686 - acc: 0.4641- ETA: 3s - lossEpoch 00006: val_loss improved from 1.83951 to 1.65385, saving model to /output/training_logs/run-4/checkpoint-06-1.6539.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.7687 - acc: 0.4644 - val_loss: 1.6539 - val_acc: 0.4961\n",
      "Epoch 8/10000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 1.6695 - acc: 0.4979Epoch 00007: val_loss improved from 1.65385 to 1.57459, saving model to /output/training_logs/run-4/checkpoint-07-1.5746.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.6674 - acc: 0.4996 - val_loss: 1.5746 - val_acc: 0.5586\n",
      "Epoch 9/10000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 1.6140 - acc: 0.5181Epoch 00008: val_loss improved from 1.57459 to 1.42751, saving model to /output/training_logs/run-4/checkpoint-08-1.4275.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.6132 - acc: 0.5183 - val_loss: 1.4275 - val_acc: 0.5840\n",
      "Epoch 10/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.5063 - acc: 0.5452Epoch 00009: val_loss improved from 1.42751 to 1.26424, saving model to /output/training_logs/run-4/checkpoint-09-1.2642.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.5063 - acc: 0.5455 - val_loss: 1.2642 - val_acc: 0.6426\n",
      "Epoch 11/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.4337 - acc: 0.5690Epoch 00010: val_loss did not improve\n",
      "256/256 [==============================] - 17s - loss: 1.4337 - acc: 0.5690 - val_loss: 1.3795 - val_acc: 0.6191\n",
      "Epoch 12/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.3803 - acc: 0.5897- EEpoch 00011: val_loss improved from 1.26424 to 1.23836, saving model to /output/training_logs/run-4/checkpoint-11-1.2384.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.3793 - acc: 0.5898 - val_loss: 1.2384 - val_acc: 0.6484\n",
      "Epoch 13/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.2915 - acc: 0.6162Epoch 00012: val_loss did not improve\n",
      "256/256 [==============================] - 17s - loss: 1.2914 - acc: 0.6163 - val_loss: 1.2903 - val_acc: 0.6211\n",
      "Epoch 14/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.2585 - acc: 0.6184Epoch 00013: val_loss improved from 1.23836 to 1.22998, saving model to /output/training_logs/run-4/checkpoint-13-1.2300.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.2592 - acc: 0.6182 - val_loss: 1.2300 - val_acc: 0.6328\n",
      "Epoch 15/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.2083 - acc: 0.6324Epoch 00014: val_loss improved from 1.22998 to 1.12502, saving model to /output/training_logs/run-4/checkpoint-14-1.1250.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.2100 - acc: 0.6315 - val_loss: 1.1250 - val_acc: 0.6855\n",
      "Epoch 16/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.1895 - acc: 0.6422Epoch 00015: val_loss did not improve\n",
      "256/256 [==============================] - 17s - loss: 1.1900 - acc: 0.6422 - val_loss: 1.1486 - val_acc: 0.6523\n",
      "Epoch 17/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.1267 - acc: 0.6643Epoch 00016: val_loss improved from 1.12502 to 1.06485, saving model to /output/training_logs/run-4/checkpoint-16-1.0648.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.1251 - acc: 0.6649 - val_loss: 1.0648 - val_acc: 0.6738\n",
      "Epoch 18/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.0925 - acc: 0.6755Epoch 00017: val_loss did not improve\n",
      "256/256 [==============================] - 17s - loss: 1.0928 - acc: 0.6754 - val_loss: 1.0653 - val_acc: 0.6699\n",
      "Epoch 19/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.0709 - acc: 0.6799Epoch 00018: val_loss improved from 1.06485 to 0.92009, saving model to /output/training_logs/run-4/checkpoint-18-0.9201.hdf5\n",
      "256/256 [==============================] - 17s - loss: 1.0716 - acc: 0.6796 - val_loss: 0.9201 - val_acc: 0.7305\n",
      "Epoch 20/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.0201 - acc: 0.6949Epoch 00019: val_loss did not improve\n",
      "256/256 [==============================] - 17s - loss: 1.0207 - acc: 0.6948 - val_loss: 0.9711 - val_acc: 0.7344\n",
      "Epoch 21/10000\n",
      "255/256 [============================>.] - ETA: 0s - loss: 1.0270 - acc: 0.6902- ETA: 0s - loss: 1.0306 - acc: 0Epoch 00020: val_loss did not improve\n",
      "256/256 [==============================] - 17s - loss: 1.0293 - acc: 0.6895 - val_loss: 0.9947 - val_acc: 0.7148\n",
      "Epoch 22/10000\n",
      "254/256 [============================>.] - ETA: 0s - loss: 0.9830 - acc: 0.7061- ETA: 3 - ETA: 1s - loss: 0.9770 -  - ETA: 0s - loss: 0.9781 -Epoch 00021: val_loss did not improve\n",
      "256/256 [==============================] - 17s - loss: 0.9816 - acc: 0.7067 - val_loss: 0.9630 - val_acc: 0.7285\n",
      "Epoch 00021: early stopping\n"
     ]
    }
   ],
   "source": [
    "# 正反 LSTM\n",
    "m = get_cnn_rnn_model()\n",
    "m.summary()\n",
    "do_train(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
